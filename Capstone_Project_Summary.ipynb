{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Analytics tables for i94 immigration dataset in 2016\n",
    "\n",
    "This project creates a proof-of-concept (POC) data lake for analysts to analyse the immigration and visiting patterns into (and out of) the USA easily. The primary i94 immigration dataset contains over 40 million rows of i94 arrival records to analyse (or roughly 3.3 million rows per month for the 12 months).\n",
    "\n",
    "## What is the i94 form for? \n",
    "\n",
    "[According to estaform.org](https://www.estaform.org/faqs/what-is-the-i-94-form#:~:text=Form%20I%2D94%2C%20also%20known,citizens%20or%20lawful%20permanent%20residents.), form I-94, also known as the Arrival-Departure Record Card, is a form used by the U.S. Customs and Border Protection (CBP) in order to keep track of the arrival and departure to and from the United States or people who are not U.S. citizens or lawful permanent residents\n",
    "\n",
    "So, the most accurate terminology of each individual record of the i94 dataset appear to be **Arrival-Departure Record** for non US citizens or lawful permanent residents. A less mouthful term might be just, a \"visit\". (in this project we shall stick with the term \"visit\" for simplicity). You may hear people use the term \"immigrant\" which we do no believe is accurate, because the total arrival-departure visits, may be related to immigration as well as non-immigration.\n",
    "\n",
    "A person may enter and depart the US multiple times during a month. A record in the datset represents a visit (into the US), whether the person might or might not have a specified departure date. If a person enter the US 3 times (departures might be 2 or 3 times) within one month, then that would should conceptually be represented by 3 records in the i94 dataset. We will stick to this mind-set for the time being. If this assumption or understanding turns out to be false, we may reiterate our definition accordingly.\n",
    "\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "## Project Summary\n",
    "\n",
    "The data currently resides on the Udacity virtual file system (with the smaller files stored in this repository). In-depth Exploratory Data Analysis (EDA) have been performed using Notebooks before we start documenting using this notebook (See `EDA_*.ipynb`). Therefore, a lot of the decisions we have made (such as scope, directory structure, and ETL scripts, have been run at least once before. This notebook acts as place to document and illustrate our \"final decision point\". i.e. a Project Summary.\n",
    "\n",
    "## AWS or no AWS?\n",
    "\n",
    "We decided that for this particular project to focus on EDA and Data Modeling on the readily available Udacity Virtual Spark Cluster. For the migration to AWS environment that would be done in next phase (out of scope of this project for now - though this might change).\n",
    "\n",
    "All inputs and outputs will be stored on the Udacity Virtual Spark Cluster. This code repository however would give you an idea of where things are, and how we might migrate to an AWS environment in future. As we retrieve local directory and file paths from the `aws_dev.cfg`, we expect the AWS solution may use the same code repository but with the pointer to the AWS paths from that config file.\n",
    "\n",
    "These tables can be used by Anayltics teams to further explore patterns and gain insights.\n",
    "\n",
    "As our initial main focus of this project are Spark, EDA (Exploratory analysis) and ETL using PySpark running on the Udacity Virtual Hadoop/Spark cluster. There will be no AWS elements in this project however further tweak may be done in future to migrate project to AWS environment (e.g. to run on EMR Spark Cluster).\n",
    "\n",
    "The project follows the follow steps:\n",
    "\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import pyspark\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Ensure Jupyter Notebook display pandas dataframe fully. Show all columns. Do not truncate column value.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope and Description of the Data\n",
    "\n",
    "Original Raw Data:\n",
    "\n",
    "1. i94 Immigration Data (2016) - SAS Datasets\n",
    "    * File format: `.SAS7BDAT` SAS Binary Data File.\n",
    "    * Content: contains over 40 million arrival (into USA) records from the i94 forms during 2016.\n",
    "    * Remark: this is our primary table. (contains fact and dimension data)\n",
    "    * Official source location: [US National Tourism and Trade Office.](https://travel.trade.gov/research/reports/i94/historical/2016.html). Note: it appears that the URL is now re-directed to another location. For this reason, we will stick with the Udacity Path below (that contains the downloaded dataset).\n",
    "    * Udacity Directory: `/data/18-83510-I94-Data-2016`. It contains 12 SAS Dataset (1 for each arrival month during 2016). Each SAS Dataset has a naming convention like this: `i94_<mmm>16_sub.sas7bdat`, where `mmm` is in this list: `['apr','aug','dec','feb','jan','jul','jun','mar','may','nov','oct','sep']`.\n",
    "    \n",
    "2. A SAS Program file that contains the lookup of codes to categorical string values\n",
    "    * File format: `.SAS` Text File (containing SAS Code Syntax)\n",
    "    * Content: this is a semi-structured text file that contains lookup logic from (numeric or character) codes to categorical string values, for the i94 Immigration Dataset columns: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`.\n",
    "    * Remark: we will have an automated Python script to read in this text file, and auto derive CSV lookup tables (as descibed in bullet point 4 below).    \n",
    "    * Official source location (no documentations identified. Likely come from the same source as source (1) above.)\n",
    "    * Relative Path: `raw_input_data/i94_proc_format_code_sas/I94_SAS_Labels_Descriptions.SAS`\n",
    "    \n",
    "3. US Airport Codes\n",
    "    * File format: `.CSV` File (Delimiter: comma `,`)\n",
    "    * Content: contains the airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia).\n",
    "    * Remark: we do not use this dataset yet. But we may in future.\n",
    "    * Official source location: [Airport Codes data by datahub.io](https://datahub.io/core/airport-codes#data)\n",
    "    * Relative path: `raw_input_data/airport_codes_csv/airport-codes_csv.csv`\n",
    "    \n",
    "Derived Raw Data (to be derived from original raw data with Python Script downstream):\n",
    "\n",
    "4. i94 Lookup CSV Files\n",
    "    * File format: `.CSV` File (Delimiter: comma `,`)\n",
    "    * Content: contains tabular lookup mapping between the (numeric or character) codes and categorical string values. This covers the i94 Immigration Dataset columns: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`. These are dimension tables.\n",
    "    * Remark: we will derive these CSV lookup tables with a Python script, using source (2 - SAS program file) above as input.\n",
    "    * Official source location: we generate these lookup tables from source (2) the SAS Program Text file as part of this project. We \n",
    "    * Relative path: `raw_input_data/i94_lookup_csv/lookup_<field>.csv`, where `<field>` may be:\n",
    "        * `i94addr`\n",
    "        * `i94cntyn` - may be used to lookup: `i94cit`, `i94res`\n",
    "        * `i94mode`\n",
    "        * `i94port`\n",
    "        * `i94visa`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "\n",
    "For each data source, we will do the followings:\n",
    "\n",
    "1. Read the data (into memory), and if suitable, write out to disk (for ease of re-retrival).\n",
    "2. Explore the Data: identify things like\n",
    "    * patterns (data structure, unique keys, possible joins, etc.)\n",
    "    * data quality issues (like missing values, duplicate data, etc.)\n",
    "    * assumptions and gotchas\n",
    "3. Document steps necessary to transform and/or clean the data to make downstream processes easier and more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Setup a Spark Session\n",
    "\n",
    "We will be using PySpark heavily to analyse and manipulate large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read Config File\n",
    "\n",
    "Read the config file `aws_dev.cfg` into a Python object. We may retrieve data from this object later on a as needed basis, such as directory and file paths, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config_dev = configparser.ConfigParser()\n",
    "config_dev.read_file(open('aws_dev.cfg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Assess Source 1: i94 Immigration Data (2016) - From SAS Datasets\n",
    "\n",
    "Overall steps:\n",
    "\n",
    "1. Reset our i94 parquet base directory (remove everything there if there are files there)\n",
    "2. Read i94 Immigration Data from `/data/18-83510-I94-Data-2016/i94_<mmm>16_sub.sas7bdat` into a Spark DataFrame\n",
    "3. Load Spark DataFrame into our base directory (where we wish the parquet format files to be stored). We can choose to partition the parquet files for ease of subset retrievals.\n",
    "4. Test retrieval the parquet files (our raw i94 \"datalake\").\n",
    "5. run some EDA here. Take observations, make and test hypothesis.\n",
    "\n",
    "References:\n",
    "\n",
    "* On bullet 1-3 above (Parse sas7bdat files to partitioned parquet files): `EDA_002_local_sas2par_i94.ipynb`\n",
    "* On bullet 4 above (Test retrieve partitioned parquet files): `EDA_008_local_analysis_par_i94_by_i94yr_i94mon_v2.ipynb`\n",
    "* On bullet 5 above (EDA): \n",
    "    * i94 fields simple groupby and write out to CSVs: `EDA_003_local_analyse_par_i94_v3.ipynb`,\n",
    "    * convert SAS dates to Python Datetime: `EDA_003_local_analyse_par_i94_v4.ipynb`\n",
    "    * for curiosity sake: relationship between i94mode and i94port?: `EDA_003_local_analyse_par_i94_v5.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_I94_MTHLY_SAS7BDAT_DIR: /data/18-83510-I94-Data-2016\n",
      "PAR_I94_DIR_BY_I94YR_I94MON: par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n"
     ]
    }
   ],
   "source": [
    "RAW_I94_MTHLY_SAS7BDAT_DIR = config_dev.get('DATA_PATHS_UDACITY', 'RAW_I94_MTHLY_SAS7BDAT_DIR')\n",
    "PAR_I94_DIR_BY_I94YR_I94MON = config_dev.get('DATA_PATHS_LOCAL', 'PAR_I94_DIR_BY_I94YR_I94MON')\n",
    "\n",
    "print(f\"RAW_I94_MTHLY_SAS7BDAT_DIR: {RAW_I94_MTHLY_SAS7BDAT_DIR}\")\n",
    "print(f\"PAR_I94_DIR_BY_I94YR_I94MON: {PAR_I94_DIR_BY_I94YR_I94MON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reset our i94 parquet base directory (remove everything there if there are files there)\n",
    "!rm -r {PAR_I94_DIR_BY_I94YR_I94MON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6239956\n",
      "-rw-r--r-- 1 root root 471990272 May 31  2018 i94_apr16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 625541120 May 31  2018 i94_aug16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 523304960 May 31  2018 i94_dec16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 391905280 May 31  2018 i94_feb16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 434176000 May 31  2018 i94_jan16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 650117120 May 31  2018 i94_jul16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 716570624 May 31  2018 i94_jun16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 481296384 May 31  2018 i94_mar16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 525008896 May 31  2018 i94_may16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 444334080 May 31  2018 i94_nov16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 556269568 May 31  2018 i94_oct16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 569180160 May 31  2018 i94_sep16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at our input directory where the sas7bdat files are stored\n",
    "!ls -l {RAW_I94_MTHLY_SAS7BDAT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def i94_sas7bdat_to_parquet(\n",
    "    spark,\n",
    "    sas7bdat_base_path: str,\n",
    "    parquet_base_path: str,\n",
    "    yy_list: list,\n",
    "    mmm_list: list,\n",
    "    partition_by_list: list = None):\n",
    "    \"\"\" Given a spark session handle and parameters, parse i94 SAS datasets into parquet directly \"\"\"\n",
    "    \n",
    "    for yy in yy_list:\n",
    "        for mmm in mmm_list:\n",
    "            input_dir = f\"{sas7bdat_base_path}\"\n",
    "            input_file = f\"i94_{mmm}{yy}_sub.sas7bdat\"\n",
    "            input_path = f\"{input_dir}/{input_file}\"\n",
    "\n",
    "            print(f\"Reading (input_path): {input_path}\")\n",
    "            df_spark_i94 = spark.read.format('com.github.saurfang.sas.spark').load(input_path)\n",
    "\n",
    "            print(f\"Writing to (parquet_base_path): {parquet_base_path}\")\n",
    "            if partition_by_list:\n",
    "                df_spark_i94.write.mode(\"append\").partitionBy(*partition_by_list).parquet(parquet_base_path) \n",
    "            else:\n",
    "                df_spark_i94.write.mode(\"append\").parquet(parquet_base_path) \n",
    "                \n",
    "    print(f\"Done. i94 dataset written to: {parquet_base_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Reading (input_path): /data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat\n",
      "Writing to (parquet_base_path): par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n",
      "Done. i94 dataset written to: par_input_data/i94/i94_parquet/by_i94yr_i94mon/\n"
     ]
    }
   ],
   "source": [
    "# Uncomment below to (re)run\n",
    "i94_sas7bdat_to_parquet(\n",
    "    spark,\n",
    "    sas7bdat_base_path=RAW_I94_MTHLY_SAS7BDAT_DIR,\n",
    "    parquet_base_path=PAR_I94_DIR_BY_I94YR_I94MON,\n",
    "    yy_list = ['16'],\n",
    "    mmm_list = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'],\n",
    "    partition_by_list = [\"i94yr\", \"i94mon\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's test retrieve the i94 parquet files. Let's retrieve the data for the whole 12 months in 2016\n",
    "\n",
    "References: `EDA_008_local_analysis_par_i94_by_i94yr_i94mon_v2.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all partitions (no filtering)\n",
    "df_i94 = spark.read.parquet(PAR_I94_DIR_BY_I94YR_I94MON)\n",
    "\n",
    "# View Schema (notice that the partitionBy columns appear at the end of the dataset)\n",
    "df_i94.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns: 28\n",
      "Total Rows: 40790529\n"
     ]
    }
   ],
   "source": [
    "# Total Column row (instant response)\n",
    "print(f\"Total Columns: {len(df_i94.columns)}\")\n",
    "\n",
    "# Total row count (takes significant less time to compute if properly partitioned)\n",
    "df_i94_rows = df_i94.count()\n",
    "print(f\"Total Rows: {df_i94_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+------------------+\n",
      "| i94yr|i94mon|records|   percent_records|\n",
      "+------+------+-------+------------------+\n",
      "|2016.0|   1.0|2847924| 6.981826590187149|\n",
      "|2016.0|   2.0|2570543| 6.301813344955639|\n",
      "|2016.0|   3.0|3157072|7.7397182076260895|\n",
      "|2016.0|   4.0|3096313| 7.590764513007418|\n",
      "|2016.0|   5.0|3444249| 8.443746831525525|\n",
      "|2016.0|   6.0|3574989| 8.764262410031504|\n",
      "|2016.0|   7.0|4265031|10.455934513622022|\n",
      "|2016.0|   8.0|4103570|10.060104883660616|\n",
      "|2016.0|   9.0|3733786| 9.153561112188568|\n",
      "|2016.0|  10.0|3649136|  8.94603744903627|\n",
      "|2016.0|  11.0|2914926| 7.146085308185143|\n",
      "|2016.0|  12.0|3432990| 8.416144835974057|\n",
      "+------+------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute monthly count stats\n",
    "df_i94.createOrReplaceTempView(\"df_i94\")\n",
    "df_i94_agg_yr_mon = spark.sql(f\"\"\"\n",
    "select\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    count(*) as records,\n",
    "    count(*) / {df_i94_rows} * 100 as percent_records\n",
    "from df_i94\n",
    "group by 1, 2\n",
    "order by 1, 2\n",
    "\"\"\")\n",
    "df_i94_agg_yr_mon.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa2ebc91fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa319892be0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise monthly count stats\n",
    "df_i94_agg_yr_mon.toPandas()\\\n",
    "    .plot.bar(x='i94mon', y='records', color='royalblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Mini observation: highest number of records during July 2016 (summer period) and lowest during Feb 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94: Data Dictionary of the raw i94 immigration monthly dataset\n",
    "\n",
    "Fields and descriptions of the monthly i94 dataset, based on EDA analysis conducted from all the `EDA_*.ipynb` Notebook analysis and research conducted so far. This Dictionary might contain mistakes, mis-interpretations, mis-assumptions, and flaws. This is a starting point dictionary nevertheless, which we can gradually improve as we conduct further EDA and research.\n",
    "\n",
    "Note that the global unique ID (GUID) is a combination of `i94yr`, `i94mon`, and `cicid`. This is validated by EDA `EDA_003_local_analyse_par_i94_v3.ipynb`.\n",
    "\n",
    "Note also that we will generate those \"lookup CSV\" files at a later phase in the Notebook. These Lookup CSV will be parsed from data source 2 (`raw_input_data/i94_proc_format_code_sas/I94_SAS_Labels_Descriptions.SAS`)\n",
    "\n",
    "Fields in the i94 dataset:\n",
    "\n",
    "* cicid : CICID is an unique ID for each US arrival-departure record which the visitor has filled in an i94 form. All arrival-departure CICID is unique. CICID is reset to 1 at the 1st of each arrival month.\n",
    "* i94yr : 4 digit year (of the arrival date into the US)\n",
    "* i94mon : Numeric month (of the arrival date into the US)\n",
    "* i94cit : code. Visitor's country of citizenship. (i.e. as per passport). (see `raw_input_data/i94_lookup_csv/lookup_i94cntyl.csv`)\n",
    "* i94res : code. Visitor's country of residence. (i.e. country travelling into the US from). (see `raw_input_data/i94_lookup_csv/lookup_i94cntyl.csv`)\n",
    "* i94port: (3-letter code) port of entry into the US. (see `raw_input_data/i94_lookup_csv/lookup_i94port.csv`)\n",
    "* arrdate: is date of arrrival (into the US). Currently in SAS Datetime format. We need to convert it to Python timestamp format.\n",
    "* i94mode: mode of travelling into the US (sea, land, air, or not reported). Not appear to correlate to i94port (based on analysis). (see `raw_input_data/i94_lookup_csv/lookup_i94mode.csv`)\n",
    "* i94addr: Visitor's residence (or accomodation) in the US, represented in 1-2 character codes. (not sure schema of code however). (see `raw_input_data/i94_lookup_csv/lookup_i94addr.csv`)\n",
    "* depdate: is date of departure (out of the US). Currently in SAS Datetime format. We need to convert it to Python timestamp format.\n",
    "* i94bir : Age of Respondent in Years.\n",
    "* i94visa : (see `raw_input_data/i94_lookup_csv/lookup_i94visa.csv`)\n",
    "* count : appears to be always 1. A classic approach to enable aggregate by summing to get the same effect as counting.\n",
    "* dtadfile : Character Date Field - Date added to I-94 Files - CIC does not use\n",
    "* visapost : Department of State where where Visa was issued - CIC does not use\n",
    "* occup : Occupation that will be performed in U.S. - CIC does not use\n",
    "* entdepa : Arrival Flag - admitted or paroled into the U.S. - CIC does not use\n",
    "* entdepd : Departure Flag - Departed, lost I-94 or is deceased - CIC does not use\n",
    "* entdepu : Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use\n",
    "* matflag : Match flag - Match of arrival and departure records\n",
    "* biryear : birth year of the immigrant on the i94 form.\n",
    "* dtaddto : Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use.\n",
    "* gender : Non-immigrant sex\n",
    "* insnum : INS number. (Is this Insurance Number?)\n",
    "* airline : Airline used to arrive in U.S.\n",
    "* admnum : Every I-94 record has an eleven-digit admission number. This number may be needed at the Department of Motor Vehicles and for employment purposes, but it is not a number that needs to be memorized. In fact, a new I-94 number will be given each time the student or scholar re-enters the United States.\n",
    "* fltno : Flight number of Airline used to arrive in U.S.\n",
    "* visatype : is the type of visa which one owns (see `raw_input_data/i94_lookup_csv/lookup_i94visa.csv`)\n",
    "\n",
    "References:\n",
    "\n",
    "* https://internationaloffice.berkeley.edu/immigration/i94\n",
    "* https://knowledge.udacity.com/questions/185453\n",
    "* https://knowledge.udacity.com/questions/548968\n",
    "* https://knowledge.udacity.com/questions/504012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def simple_groupby(spark, df, colname: str, ordered: bool = True):\n",
    "    \"\"\" Given a Spark Session, Spark DataFrame and a group-by field, \n",
    "        do a simple count / groupby (one field only)\n",
    "        Return a Spark DataFrame\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView(\"_tmp_df\")\n",
    "    sql_str = f\"\"\"\\\n",
    "        select {colname}, count(*) as records\n",
    "        from _tmp_df\n",
    "        group by 1\n",
    "    \"\"\"\n",
    "    df2 = spark.sql(sql_str)\n",
    "    if ordered:\n",
    "        df2=df2.orderBy(colname)       \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### cicid\n",
    "\n",
    "CIC-ID. Unique ID of each visit. It is unique per Arrival year (`i94yr`) and month (`i94mon`).\n",
    "\n",
    "Hypothesis and observations following analysis:\n",
    "\n",
    "* Hypothesis (Validated): is that cicid is unique per arrival year-month.\n",
    "* Hypothesis (Validated): total records per cicid could be between 1 and 12 (for the 12 months in 2016).\n",
    "* Hypothesis (Both invalidated but it's ok): the total monthly count for each month, must be equal to the max cicid value for the month. And the min CICID is 1 for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|cicid|records|\n",
      "+-----+-------+\n",
      "|  1.0|      1|\n",
      "|  2.0|      4|\n",
      "|  3.0|      3|\n",
      "|  4.0|      3|\n",
      "|  5.0|      5|\n",
      "|  6.0|      6|\n",
      "|  7.0|      7|\n",
      "|  8.0|      5|\n",
      "|  9.0|      4|\n",
      "| 10.0|      2|\n",
      "+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cicid_agg_1 = simple_groupby(spark, df_i94, 'cicid')\n",
    "df_cicid_agg_1.show(10)\n",
    "\n",
    "# Unfortunately the tail() method is not available until PySPark version 3.\n",
    "# so have to use the less efficient descending sort! (to see largest values)\n",
    "# df_cicid_agg_1.tail(10)\n",
    "# df_cicid_agg_1.sort(F.col('cicid').desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-------+\n",
      "|cicid|i94yr|i94mon|records|\n",
      "+-----+-----+------+-------+\n",
      "+-----+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My hypothesis is that cicid is unique per arrival year-month.\n",
    "# if my hypothesis is correct then the following query should requrn 0 rows\n",
    "# Result indicated: hypothesis is validated\n",
    "spark.sql(\"\"\"\\\n",
    "select\n",
    "    cicid,\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    count(*) as records\n",
    "from df_i94\n",
    "group by 1, 2, 3\n",
    "having records > 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|    cicid|records|\n",
      "+---------+-------+\n",
      "|5680982.0|      7|\n",
      "|5681018.0|      6|\n",
      "|5681279.0|      6|\n",
      "|5681289.0|      8|\n",
      "|5681452.0|      5|\n",
      "|5681523.0|      8|\n",
      "|5681771.0|      6|\n",
      "|5682204.0|      3|\n",
      "|5682909.0|      7|\n",
      "|5683017.0|      7|\n",
      "|5683674.0|      6|\n",
      "|5684532.0|      5|\n",
      "|5684559.0|      3|\n",
      "|5684829.0|      5|\n",
      "|5685062.0|      6|\n",
      "|5685608.0|      5|\n",
      "|5686057.0|      7|\n",
      "|5686102.0|      7|\n",
      "|5686163.0|      7|\n",
      "|5686168.0|      4|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My hypothesis is that cicid is unique per arrival year-month.\n",
    "# if my hypothesis is correct then the following query should requrn some rows\n",
    "# Result indicated: hypothesis is validated\n",
    "spark.sql(\"\"\"\\\n",
    "select\n",
    "    cicid,\n",
    "    count(*) as records\n",
    "from df_i94\n",
    "group by 1\n",
    "having records > 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min_records|max_records|\n",
      "+-----------+-----------+\n",
      "|          1|         12|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what's the min and max number of records per CICID (during 2016?)\n",
    "# My hypothesis is that CICID resets to 1 at the beginning of every month.\n",
    "# i.e. for a period of 12 months, I would expect the min is 1, and the max is 12.\n",
    "# Result suggests: hypothesis validated.\n",
    "spark.sql(\"\"\"\\\n",
    "select\n",
    "    min(records) as min_records,\n",
    "    max(records) as max_records\n",
    "from (\n",
    "    select\n",
    "        cicid,\n",
    "        count(*) as records\n",
    "    from df_i94\n",
    "    group by 1\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+-------+\n",
      "| i94yr|i94mon|min_cic_id|max_cic_id|records|\n",
      "+------+------+----------+----------+-------+\n",
      "|2016.0|   1.0|       7.0| 6148395.0|2847924|\n",
      "|2016.0|   2.0|       2.0| 5507053.0|2570543|\n",
      "|2016.0|   3.0|       2.0| 6757651.0|3157072|\n",
      "|2016.0|   4.0|       6.0| 6102785.0|3096313|\n",
      "|2016.0|   5.0|       2.0| 6484857.0|3444249|\n",
      "|2016.0|   6.0|       4.0| 6432838.0|3574989|\n",
      "|2016.0|   7.0|       1.0| 7667577.0|4265031|\n",
      "|2016.0|   8.0|      22.0| 7231273.0|4103570|\n",
      "|2016.0|   9.0|     155.0| 6733429.0|3733786|\n",
      "|2016.0|  10.0|       5.0| 6403017.0|3649136|\n",
      "|2016.0|  11.0|       8.0| 5812458.0|2914926|\n",
      "|2016.0|  12.0|       6.0| 7318723.0|3432990|\n",
      "+------+------+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the min and max cicid for each month?\n",
    "# if cicid is indeed reset at 1 at beginning of each month, and incrementally increasing/\n",
    "# then the total monthly cound for each month, must be equal to the max cicid (the hypothesis).\n",
    "# also, the min CICID is 1 for each month.\n",
    "# Result suggests: hypothesis invalidated\n",
    "# (But this is ok, as long as CICID is unique per month, which we have validated, we are ok.)\n",
    "spark.sql(f\"\"\"\n",
    "select\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    min(cicid) as min_cic_id,\n",
    "    max(cicid) as max_cic_id,\n",
    "    count(*) as records\n",
    "from df_i94\n",
    "group by 1, 2\n",
    "order by 1, 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94yr\n",
    "\n",
    "i94yr - 4 digit year (of the arrival date into the US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "| i94yr| records|\n",
      "+------+--------+\n",
      "|2016.0|40790529|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94yr_agg_1 = simple_groupby(spark, df_i94, 'i94yr')\n",
    "df_i94yr_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94mon\n",
    "\n",
    "i94mon - Numeric month (of the arrival date into the US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|i94mon|records|\n",
      "+------+-------+\n",
      "|   1.0|2847924|\n",
      "|   2.0|2570543|\n",
      "|   3.0|3157072|\n",
      "|   4.0|3096313|\n",
      "|   5.0|3444249|\n",
      "|   6.0|3574989|\n",
      "|   7.0|4265031|\n",
      "|   8.0|4103570|\n",
      "|   9.0|3733786|\n",
      "|  10.0|3649136|\n",
      "|  11.0|2914926|\n",
      "|  12.0|3432990|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94mon_agg_1 = simple_groupby(spark, df_i94, 'i94mon')\n",
    "df_i94mon_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94cit and i94res\n",
    "\n",
    "This format shows all the valid and invalid (country) codes for processing\n",
    "\n",
    "* `i94cit` - Visitor's country of citizenship. (i.e. as per passport).\n",
    "* `i94res` - Visitor's country of residence. (i.e. country travelling into the US from).\n",
    "\n",
    "Idea: parse lookup table (PROC FORMAT) from `I94_SAS_Labels_Descriptions.SAS` into a structured dataset (e.g. tabular dataframe / csv) for downstream processing.\n",
    "\n",
    "Rationale for idea: we may potentially map `i94cit` and `i94res` (numeric code) to some meaningful string values (country).\n",
    "\n",
    "Observation:\n",
    "\n",
    "* Key: integer code (1-3 digits)\n",
    "* Value: country name, or state name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|i94cit|records|\n",
      "+------+-------+\n",
      "|  null|  28575|\n",
      "|   0.0|   1288|\n",
      "| 101.0|  11424|\n",
      "| 102.0|   1287|\n",
      "| 103.0| 203883|\n",
      "| 104.0| 270440|\n",
      "| 105.0|  30101|\n",
      "| 106.0|      5|\n",
      "| 107.0| 190060|\n",
      "| 108.0| 328086|\n",
      "| 109.0|  23583|\n",
      "| 110.0| 154249|\n",
      "| 111.0|1679312|\n",
      "| 112.0|  10537|\n",
      "| 113.0|  82025|\n",
      "| 114.0|  85107|\n",
      "| 115.0|  58268|\n",
      "| 116.0| 476312|\n",
      "| 117.0|1116790|\n",
      "| 118.0|  20870|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94cit_agg_1 = simple_groupby(spark, df_i94, 'i94cit')\n",
    "df_i94cit_agg_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|i94res|records|\n",
      "+------+-------+\n",
      "| 101.0|  12317|\n",
      "| 102.0|   1589|\n",
      "| 103.0| 199332|\n",
      "| 104.0| 276722|\n",
      "| 105.0|  28080|\n",
      "| 107.0| 180847|\n",
      "| 108.0| 327074|\n",
      "| 109.0|  22441|\n",
      "| 110.0| 150704|\n",
      "| 111.0|1641152|\n",
      "| 112.0|2046288|\n",
      "| 113.0|  74444|\n",
      "| 114.0|  78625|\n",
      "| 115.0|  57856|\n",
      "| 116.0| 443005|\n",
      "| 117.0| 988860|\n",
      "| 118.0|  19420|\n",
      "| 119.0|   2082|\n",
      "| 120.0|  27660|\n",
      "| 121.0|  19360|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94res_agg_1 = simple_groupby(spark, df_i94, 'i94res')\n",
    "df_i94res_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94port\n",
    "\n",
    "i94port - (3-letter code) port of entry into the US. This format shows all the valid and invalid codes for processing\n",
    "\n",
    "Idea: parse lookup table (PROC FORMAT) from `I94_SAS_Labels_Descriptions.SAS` into a structured dataset (e.g. tabular dataframe / csv) for downstream processing.\n",
    "\n",
    "Rationale for idea: we may potentially map `i94port` (3 character code) to some meaningful string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|i94port|records|\n",
      "+-------+-------+\n",
      "|    48Y|      2|\n",
      "|    5KE|     58|\n",
      "|    5T6|    161|\n",
      "|    74S|      1|\n",
      "|    ABE|      2|\n",
      "|    ABG|    527|\n",
      "|    ABQ|     61|\n",
      "|    ABS|    264|\n",
      "|    ACY|      5|\n",
      "|    ADS|    388|\n",
      "|    ADT|    124|\n",
      "|    ADW|    505|\n",
      "|    AFW|     19|\n",
      "|    AGA|1337940|\n",
      "|    AGM|      2|\n",
      "|    AGN|    163|\n",
      "|    AGU|     12|\n",
      "|    ALB|     49|\n",
      "|    ALC|   3976|\n",
      "|    ANA|      2|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94port_agg_1 = simple_groupby(spark, df_i94, 'i94port')\n",
    "df_i94port_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### arrdate and depdate\n",
    "\n",
    "Both are in SAS Datetime format. We need to convert it to Python timestamp format.\n",
    "\n",
    "* `arrdate` - Date of arrrival (into the US).\n",
    "* `depdate` - Date of arrrival (into the US). \n",
    "\n",
    "Reference: `EDA_005_local_analyse_par_i94_v4.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|arrdate|records|\n",
      "+-------+-------+\n",
      "|20454.0|  83984|\n",
      "|20455.0|  75646|\n",
      "|20456.0|  79959|\n",
      "|20457.0|  89325|\n",
      "|20458.0|  85650|\n",
      "|20459.0|  84420|\n",
      "|20460.0|  93107|\n",
      "|20461.0|  93818|\n",
      "|20462.0| 101576|\n",
      "|20463.0| 106167|\n",
      "|20464.0|  99145|\n",
      "|20465.0|  84688|\n",
      "|20466.0|  85486|\n",
      "|20467.0|  92151|\n",
      "|20468.0|  95030|\n",
      "|20469.0| 107911|\n",
      "|20470.0| 110195|\n",
      "|20471.0| 108063|\n",
      "|20472.0|  85322|\n",
      "|20473.0|  90017|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_arrdate_agg_1 = simple_groupby(spark, df_i94, 'arrdate')\n",
    "df_arrdate_agg_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "| depdate|records|\n",
      "+--------+-------+\n",
      "|    null|3308012|\n",
      "|-14388.0|      1|\n",
      "|-14375.0|      1|\n",
      "|-14359.0|      1|\n",
      "|-14342.0|      4|\n",
      "|-14334.0|      3|\n",
      "|-14328.0|      1|\n",
      "|-14321.0|      1|\n",
      "|-14312.0|      3|\n",
      "|-14307.0|     28|\n",
      "|-14283.0|      1|\n",
      "|-14021.0|      1|\n",
      "|-14011.0|      1|\n",
      "|-13977.0|      1|\n",
      "|-13888.0|      1|\n",
      "|-12285.0|      1|\n",
      "|-11972.0|      1|\n",
      "|-11950.0|      1|\n",
      "| -6717.0|      1|\n",
      "|   598.0|      1|\n",
      "|   669.0|      1|\n",
      "|   962.0|      1|\n",
      "|  4260.0|      1|\n",
      "|  4264.0|      1|\n",
      "|  4331.0|      1|\n",
      "|  7924.0|      1|\n",
      "| 14979.0|      1|\n",
      "| 14991.0|      1|\n",
      "| 15065.0|      1|\n",
      "| 15176.0|      1|\n",
      "| 15279.0|      1|\n",
      "| 16809.0|      1|\n",
      "| 17032.0|      1|\n",
      "| 17076.0|      1|\n",
      "| 17119.0|      1|\n",
      "| 17122.0|      1|\n",
      "| 17143.0|      1|\n",
      "| 17335.0|      1|\n",
      "| 18289.0|      1|\n",
      "| 18318.0|      1|\n",
      "+--------+-------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Got some negative numeric values and null\n",
    "df_depdate_agg_1 = simple_groupby(spark, df_i94, 'depdate')\n",
    "df_depdate_agg_1.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_datetime(sas_date):\n",
    "    \"\"\" Give a SAS Date, return the equivalent in Python datetime \n",
    "    Ref: https://knowledge.udacity.com/questions/66798\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (sas_date == 'null'):\n",
    "            sas_date = 0\n",
    "        start_cutoff = datetime(1960, 1, 1)\n",
    "        return start_cutoff + timedelta(days=int(sas_date))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Mini Unit Tests\n",
    "assert convert_datetime(20484) ==  datetime(2016, 1, 31)\n",
    "assert convert_datetime(20484).strftime('%Y-%m-%d') ==  '2016-01-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create two new columns in Python datetime: arrdate_pydt (from arrdate) and depdate_pydt (from depdate)\n",
    "udf_datetime_from_sas = F.udf(lambda x: convert_datetime(x), T.DateType())\n",
    "df_i94 = df_i94.withColumn('arrdate_pydt', udf_datetime_from_sas(df_i94.arrdate))\n",
    "df_i94 = df_i94.withColumn('depdate_pydt', udf_datetime_from_sas(df_i94.depdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+------------+-------+------------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|arrdate_pydt|depdate|depdate_pydt|\n",
      "+---------+------+------+------+------+-------+-------+------------+-------+------------+\n",
      "|5748517.0|2016.0|   1.0| 135.0| 135.0|    LOS|20484.0|  2016-01-31|   null|        null|\n",
      "|5748517.0|2016.0|   3.0| 135.0| 135.0|    NYC|20541.0|  2016-03-28|20545.0|  2016-04-01|\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|  2016-04-30|20582.0|  2016-05-08|\n",
      "|5748517.0|2016.0|   6.0| 252.0| 209.0|    AGA|20631.0|  2016-06-26|   null|        null|\n",
      "|5748517.0|2016.0|   7.0| 251.0| 251.0|    NYC|20659.0|  2016-07-24|20665.0|  2016-07-30|\n",
      "|5748517.0|2016.0|   8.0| 117.0| 117.0|    WAS|20691.0|  2016-08-25|20699.0|  2016-09-02|\n",
      "|5748517.0|2016.0|   9.0| 254.0| 276.0|    AGA|20723.0|  2016-09-26|20728.0|  2016-10-01|\n",
      "|5748517.0|2016.0|  10.0| 111.0| 111.0|    NYC|20755.0|  2016-10-28|20762.0|  2016-11-04|\n",
      "|5748517.0|2016.0|  11.0| 268.0| 268.0|    LOS|20788.0|  2016-11-30|20789.0|  2016-12-01|\n",
      "+---------+------+------+------+------+-------+-------+------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a peek\n",
    "# In addition, let's use this as an opportunity to test a hypothesis\n",
    "# Hypothesis (Validated): the same CICID of different monthly i94 datasets are mutually exclusive.\n",
    "# Observation: it appears that the monthly datasets are mutually exclusive to each other.\n",
    "# This means that the same CICID (say, 5748517) does not neccessary represent the same immigrant in Jan dataset, \n",
    "# vs the Feb dataset (and so on). i.e it appears that CICID is just a way for the dataset to capture immigration data.\n",
    "df_i94.createOrReplaceTempView('df_i94')\n",
    "spark.sql(\"\"\"\n",
    "select\n",
    "    cicid,\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    i94cit,\n",
    "    i94res,\n",
    "    i94port,\n",
    "    arrdate,\n",
    "    arrdate_pydt,\n",
    "    depdate,\n",
    "    depdate_pydt\n",
    "from df_i94\n",
    "where cicid = 5748517\n",
    "order by 1,2,3\n",
    "limit 1000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+-------+--------+\n",
      "| i94yr|i94mon|arr_year|arr_mon|count(1)|\n",
      "+------+------+--------+-------+--------+\n",
      "|2016.0|   1.0|    2016|      1| 2847924|\n",
      "|2016.0|   2.0|    2016|      2| 2570543|\n",
      "|2016.0|   3.0|    2016|      3| 3157072|\n",
      "|2016.0|   4.0|    2016|      4| 3096313|\n",
      "|2016.0|   5.0|    2016|      5| 3444249|\n",
      "|2016.0|   6.0|    2016|      6| 3574989|\n",
      "|2016.0|   7.0|    2016|      7| 4265031|\n",
      "|2016.0|   8.0|    2016|      8| 4103570|\n",
      "|2016.0|   9.0|    2016|      9| 3733786|\n",
      "|2016.0|  10.0|    2016|     10| 3649136|\n",
      "|2016.0|  11.0|    2016|     11| 2914926|\n",
      "|2016.0|  12.0|    2016|     12| 3432990|\n",
      "+------+------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hypothesis (Validated): the year-month of the i94 form (`i94yr` and `i94mon`) \n",
    "# matches exactly as the arrivate date into the US (`year(arrdate_pydt)` and `month(arrdate_pydt)`)\n",
    "# Observation: It also appears that the i94yr and i94mon corresponds to the arrivate date (into the US)\n",
    "# looking at an example. (we can validate this by comparing year-month of the datast (`i94yr` and `i94mon`), vs the year-month of the arrival date (`arrdate_pydt`).\n",
    "spark.sql(\"\"\"\n",
    "select\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    year(arrdate_pydt) as arr_year,\n",
    "    month(arrdate_pydt) as arr_mon,\n",
    "    count(*)\n",
    "from df_i94\n",
    "group by 1,2,3,4\n",
    "order by 1,2,3,4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Mini summary on dates:\n",
    "\n",
    "Hypothesis 1 (Validated): the same CICID of different monthly i94 datasets are mutually exclusive.\n",
    "\n",
    "* it appears that the monthly datasets are mutually exclusive to each other.\n",
    "* This means that the same CICID (say, 5748517) does not neccessary represent the same immigrant in Jan dataset, vs the Feb dataset (and so on). i.e it appears that CICID is just a way for the dataset to capture immigration data.\n",
    "\n",
    "Hypothesis 2 (Validated): the year-month of the i94 form (`i94yr` and `i94mon`) matches exactly as the arrivate date into the US (`year(arrdate_pydt)` and `month(arrdate_pydt)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94mode\n",
    "\n",
    "i94mode - There are missing values as well as not reported.\n",
    "\n",
    "Idea: parse lookup table (PROC FORMAT) from `I94_SAS_Labels_Descriptions.SAS` into a structured dataset (e.g. tabular dataframe / csv) for downstream processing.\n",
    "\n",
    "Rationale for idea: we may potentially map `i94mode` (numeric code) to some meaningful string values (mode of transport)\n",
    "\n",
    "* Key: integer\n",
    "* Value: mode of transport:\n",
    "\n",
    "```\n",
    "    1 = 'Air'\n",
    "    2 = 'Sea'\n",
    "    3 = 'Land'\n",
    "    9 = 'Not reported'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94mode| records|\n",
      "+-------+--------+\n",
      "|   null|   73949|\n",
      "|    0.0|       4|\n",
      "|    1.0|39166088|\n",
      "|    2.0|  387184|\n",
      "|    3.0| 1095001|\n",
      "|    9.0|   68303|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94mode_agg_1 = simple_groupby(spark, df_i94, 'i94mode')\n",
    "df_i94mode_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94addr\n",
    "\n",
    "i94addr - There are lots of invalid codes in this variable and the list below shows what we have found to be valid, everything else goes into 'other'\n",
    "\n",
    "Idea: parse lookup table (PROC FORMAT) from `I94_SAS_Labels_Descriptions.SAS` into a structured dataset (e.g. tabular dataframe / csv) for downstream processing.\n",
    "\n",
    "Rationale for idea: we may potentially map `i94addr` (1-2 character code) to some meaningful string values (name of the US State or oversea territories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|i94addr|records|\n",
      "+-------+-------+\n",
      "|   null|2027926|\n",
      "|     **|      4|\n",
      "|     ..|    254|\n",
      "|     .7|      1|\n",
      "|     .9|      3|\n",
      "|     .A|      5|\n",
      "|     .C|    206|\n",
      "|     .D|     14|\n",
      "|     .F|      4|\n",
      "|     .H|      7|\n",
      "|     .I|     27|\n",
      "|     .K|      2|\n",
      "|     .L|      4|\n",
      "|     .M|     11|\n",
      "|     .N|     36|\n",
      "|     .O|      3|\n",
      "|     .S|      7|\n",
      "|     .T|     56|\n",
      "|     .V|      5|\n",
      "|     .W|      1|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94addr_agg_1 = simple_groupby(spark, df_i94, 'i94addr')\n",
    "df_i94addr_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94bir\n",
    "\n",
    "I94BIR - Age of Respondent in Years\n",
    "\n",
    "Observation: \n",
    "\n",
    "* there are some invalue values (negative 1/2/3 years old?).\n",
    "* abnormally large age? 1812 years old?\n",
    "* null should be ok as these are just unprovided data points?\n",
    "* are zero years old actually zero, or are some of these null? (some people like to use 0 to denote null)\n",
    "\n",
    "```\n",
    "+------+-------+\n",
    "|i94bir|records|\n",
    "+------+-------+\n",
    "|  null|   9517|\n",
    "|  -3.0|      1|\n",
    "|  -2.0|      2|\n",
    "|  -1.0|      7|\n",
    "...\n",
    "|1812.0|      2|\n",
    "+------+-------+\n",
    "```\n",
    "\n",
    "(Note: these abnormalities likely relates to the `biryear` analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|i94bir|records|\n",
      "+------+-------+\n",
      "|  null|   9517|\n",
      "|  -3.0|      1|\n",
      "|  -2.0|      2|\n",
      "|  -1.0|      7|\n",
      "|   0.0|  44698|\n",
      "|   1.0| 173201|\n",
      "|   2.0| 178813|\n",
      "|   3.0| 168773|\n",
      "|   4.0| 195652|\n",
      "|   5.0| 210899|\n",
      "|   6.0| 227145|\n",
      "|   7.0| 235070|\n",
      "|   8.0| 252727|\n",
      "|   9.0| 268906|\n",
      "|  10.0| 276404|\n",
      "|  11.0| 282934|\n",
      "|  12.0| 304090|\n",
      "|  13.0| 313089|\n",
      "|  14.0| 331748|\n",
      "|  15.0| 386115|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94bir_agg_1 = simple_groupby(spark, df_i94, 'i94bir')\n",
    "df_i94bir_agg_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|i94bir|records|\n",
      "+------+-------+\n",
      "|1812.0|      2|\n",
      "| 116.0|      9|\n",
      "| 115.0|      5|\n",
      "| 114.0|      4|\n",
      "| 113.0|      8|\n",
      "| 112.0|     10|\n",
      "| 111.0|     10|\n",
      "| 110.0|     11|\n",
      "| 109.0|     15|\n",
      "| 108.0|     16|\n",
      "| 107.0|     12|\n",
      "| 106.0|     19|\n",
      "| 105.0|    108|\n",
      "| 104.0|     92|\n",
      "| 103.0|     65|\n",
      "| 102.0|     77|\n",
      "| 101.0|     86|\n",
      "| 100.0|    185|\n",
      "|  99.0|    186|\n",
      "|  98.0|    253|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94bir_agg_1.sort(F.col(\"i94bir\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94visa\n",
    "\n",
    "I94VISA - Visa codes collapsed into three categories:\n",
    "\n",
    "```\n",
    "   1 -> \"Business\"\n",
    "   2 -> \"Pleasure\"\n",
    "   3 -> \"Student\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94visa| records|\n",
      "+-------+--------+\n",
      "|    1.0| 5575279|\n",
      "|    2.0|33641979|\n",
      "|    3.0| 1573271|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94visa_agg_1 = simple_groupby(spark, df_i94, 'i94visa')\n",
    "df_i94visa_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### count\n",
    "\n",
    "COUNT - Used for summary statistics.\n",
    "\n",
    "Obervation: looks like this column is always `1`. This is a classic technique for easie of aggregation (so we summing this column is equivalent as counting rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|count| records|\n",
      "+-----+--------+\n",
      "|  1.0|40790529|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count_agg_1 = simple_groupby(spark, df_i94, 'count')\n",
    "df_count_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# dtadfile\n",
    "\n",
    "DTADFILE - Character Date Field - Date added to I-94 Files - CIC does not use\n",
    "\n",
    "Observation: looks like numbers are in date like format: yyyymmdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|dtadfile|records|\n",
      "+--------+-------+\n",
      "|    null| 131050|\n",
      "|20081124|      1|\n",
      "|20120102|      1|\n",
      "|20120418|      1|\n",
      "|20120831|      1|\n",
      "|20130312|      1|\n",
      "|20130514|      1|\n",
      "|20130811|      1|\n",
      "|20140407|      1|\n",
      "|20150107|      1|\n",
      "|20150406|      1|\n",
      "|20150530|      1|\n",
      "|20150608|      1|\n",
      "|20150619|      1|\n",
      "|20150806|      1|\n",
      "|20150810|      1|\n",
      "|20150822|      1|\n",
      "|20150824|      1|\n",
      "|20150912|      1|\n",
      "|20151015|      3|\n",
      "+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dtadfile_agg_1 = simple_groupby(spark, df_i94, 'dtadfile')\n",
    "df_dtadfile_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### visapost\n",
    "\n",
    "VISAPOST - Department of State where where Visa was issued - CIC does not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|visapost| records|\n",
      "+--------+--------+\n",
      "|    null|24032175|\n",
      "|     999|    8823|\n",
      "|     ABD|   45618|\n",
      "|     ABG|     598|\n",
      "|     ABJ|    6025|\n",
      "|     ABS|     121|\n",
      "|     ABU|   43207|\n",
      "|     ACC|   16622|\n",
      "|     ACK|   16054|\n",
      "|     ADA|      31|\n",
      "|     ADD|   16650|\n",
      "|     ADL|       6|\n",
      "|     ADN|       3|\n",
      "|     ADT|       3|\n",
      "|     AGA|    1166|\n",
      "|     AKD|    1099|\n",
      "|     ALB|      19|\n",
      "|     ALC|      10|\n",
      "|     ALG|    7444|\n",
      "|     ALP|       4|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visapost_agg_1 = simple_groupby(spark, df_i94, 'visapost')\n",
    "df_visapost_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### occup\n",
    "\n",
    "OCCUP - Occupation that will be performed in U.S. - CIC does not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|occup| records|\n",
      "+-----+--------+\n",
      "| null|40597574|\n",
      "|  010|       1|\n",
      "|  035|       1|\n",
      "|  037|       1|\n",
      "|  039|       2|\n",
      "|  049|       6|\n",
      "|  093|       1|\n",
      "|  100|       1|\n",
      "|  101|       1|\n",
      "|  111|       1|\n",
      "|  120|       1|\n",
      "|  130|       1|\n",
      "|  200|       2|\n",
      "|  300|       3|\n",
      "|  430|       5|\n",
      "|  800|       1|\n",
      "|  850|       3|\n",
      "|  855|     260|\n",
      "|  992|      80|\n",
      "|  995|       1|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_occup_agg_1 = simple_groupby(spark, df_i94, 'occup')\n",
    "df_occup_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### entdepa\n",
    "\n",
    "ENTDEPA - Arrival Flag - admitted or paroled into the U.S. - CIC does not use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|entdepa| records|\n",
      "+-------+--------+\n",
      "|   null|    2404|\n",
      "|      A| 1441544|\n",
      "|      B|    1957|\n",
      "|      F|      26|\n",
      "|      G|30937479|\n",
      "|      H|  190602|\n",
      "|      I|   52706|\n",
      "|      J|     118|\n",
      "|      K|  265447|\n",
      "|      M|     251|\n",
      "|      N|    3649|\n",
      "|      O| 5692584|\n",
      "|      P|  197710|\n",
      "|      Q|     274|\n",
      "|      R|     316|\n",
      "|      T|  896198|\n",
      "|      U|   38896|\n",
      "|      Z| 1068368|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_entdepa_agg_1 = simple_groupby(spark, df_i94, 'entdepa')\n",
    "df_entdepa_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### entdepd\n",
    "\n",
    "ENTDEPA - Departure Flag - Departed, lost I-94 or is deceased - CIC does not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|entdepd| records|\n",
      "+-------+--------+\n",
      "|   null| 3287909|\n",
      "|      D| 1226934|\n",
      "|      G|   15089|\n",
      "|      I| 1395706|\n",
      "|      J|   21315|\n",
      "|      K| 1053478|\n",
      "|      L|     304|\n",
      "|      M|     223|\n",
      "|      N|  654917|\n",
      "|      O|31560000|\n",
      "|      Q|  896570|\n",
      "|      R|  551183|\n",
      "|      T|     656|\n",
      "|      V|   39257|\n",
      "|      W|   86873|\n",
      "|      Y|       2|\n",
      "|      Z|     113|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_entdepd_agg_1 = simple_groupby(spark, df_i94, 'entdepd')\n",
    "df_entdepd_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### entdepu\n",
    "\n",
    "ENTDEPU - Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|entdepu| records|\n",
      "+-------+--------+\n",
      "|   null|40777323|\n",
      "|      P|       2|\n",
      "|      U|   13156|\n",
      "|      Y|      48|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING: can take very long?\n",
    "df_entdepu_agg_1 = simple_groupby(spark, df_i94, 'entdepu')\n",
    "df_entdepu_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### matflag\n",
    "\n",
    "MATFLAG - Match flag - Match of arrival and departure records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|matflag| records|\n",
      "+-------+--------+\n",
      "|   null| 3219581|\n",
      "|      M|37570948|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_matflag_agg_1 = simple_groupby(spark, df_i94, 'matflag')\n",
    "df_matflag_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### biryear\n",
    "\n",
    "BIRYEAR - 4 digit year of birth. \n",
    "\n",
    "Observation:\n",
    "\n",
    "* 2 invalid bird year: 204?\n",
    "\n",
    "```\n",
    "+-------+-------+\n",
    "|biryear|records|\n",
    "+-------+-------+\n",
    "|  204.0|      2|\n",
    "```\n",
    "\n",
    "* Birthday in the future? (given arrival dataset is 2016):\n",
    "\n",
    "```\n",
    "+-------+-------+\n",
    "|biryear|records|\n",
    "+-------+-------+\n",
    "| 2019.0|      1|\n",
    "| 2018.0|      2|\n",
    "| 2017.0|      7|\n",
    "```\n",
    "\n",
    "(Note: these records are likely related to what we saw in the `i94bir` (age of visitor) analysis, where we saw some abnormally large value in age, and negative age.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|biryear|records|\n",
      "+-------+-------+\n",
      "|   null|   9517|\n",
      "|  204.0|      2|\n",
      "| 1900.0|      9|\n",
      "| 1901.0|      5|\n",
      "| 1902.0|      4|\n",
      "| 1903.0|      8|\n",
      "| 1904.0|     10|\n",
      "| 1905.0|     10|\n",
      "| 1906.0|     11|\n",
      "| 1907.0|     15|\n",
      "+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-------+\n",
      "|biryear|records|\n",
      "+-------+-------+\n",
      "| 2019.0|      1|\n",
      "| 2018.0|      2|\n",
      "| 2017.0|      7|\n",
      "| 2016.0|  44698|\n",
      "| 2015.0| 173201|\n",
      "| 2014.0| 178813|\n",
      "| 2013.0| 168773|\n",
      "| 2012.0| 195652|\n",
      "| 2011.0| 210899|\n",
      "| 2010.0| 227145|\n",
      "| 2009.0| 235070|\n",
      "| 2008.0| 252727|\n",
      "| 2007.0| 268906|\n",
      "| 2006.0| 276404|\n",
      "| 2005.0| 282934|\n",
      "| 2004.0| 304090|\n",
      "| 2003.0| 313089|\n",
      "| 2002.0| 331748|\n",
      "| 2001.0| 386115|\n",
      "| 2000.0| 431350|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_biryear_agg_1 = simple_groupby(spark, df_i94, 'biryear')\n",
    "df_biryear_agg_1.show(10)\n",
    "df_biryear_agg_1.sort(F.col(\"biryear\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dtaddto\n",
    "\n",
    "DTADDTO - Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use\n",
    "\n",
    "Observations:\n",
    "\n",
    "* in general numeric value appear to be in US dateformat: mmddyyyy\n",
    "* null values probably meant not applicable?\n",
    "* Contains invalid values\n",
    "\n",
    "```\n",
    "+--------+-------+\n",
    "| dtaddto|records|\n",
    "+--------+-------+\n",
    "|-00-0000|      3|\n",
    "|/   183D|      1|\n",
    "|   /184D|      1|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "| dtaddto|records|\n",
      "+--------+-------+\n",
      "|    null| 101551|\n",
      "|-00-0000|      3|\n",
      "|/   183D|      1|\n",
      "|   /184D|      1|\n",
      "|00000000|  40209|\n",
      "|01011970|      2|\n",
      "|01012016|     16|\n",
      "|01012017| 127109|\n",
      "|01012018|   2977|\n",
      "|01012019|      3|\n",
      "+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dtaddto_agg_1 = simple_groupby(spark, df_i94, 'dtaddto')\n",
    "df_dtaddto_agg_1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### gender\n",
    "\n",
    "GENDER - Non-immigrant sex\n",
    "\n",
    "Question: what are \"U\" and \"X\"?\n",
    "\n",
    "```\n",
    "+------+--------+\n",
    "|gender| Meaning|\n",
    "+------+--------+\n",
    "|     F|Female  |\n",
    "|     M|Male    |\n",
    "|     U|???     |\n",
    "|     X|???     |\n",
    "+------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender| records|\n",
      "+------+--------+\n",
      "|  null| 4079983|\n",
      "|     F|18176125|\n",
      "|     M|18504505|\n",
      "|     U|   18906|\n",
      "|     X|   11010|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gender_agg_1 = simple_groupby(spark, df_i94, 'gender')\n",
    "df_gender_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### insnum\n",
    "\n",
    "INSNUM - INS number (is this Insurance Number?)\n",
    "\n",
    "Observations:\n",
    "\n",
    "* appear numeric in general with zero padded on left.\n",
    "* abnormal values:\n",
    "\n",
    "```\n",
    "+------+--------+\n",
    "|insnum| records|\n",
    "+------+--------+\n",
    "|     '|       5|\n",
    "| '2868|       1|\n",
    "| '3884|       1|\n",
    "| .3434|       1|\n",
    "| .3890|       1|\n",
    "| /3685|       1|\n",
    "|     0|       6|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|insnum| records|\n",
      "+------+--------+\n",
      "|  null|35678095|\n",
      "|     '|       5|\n",
      "| '2868|       1|\n",
      "| '3884|       1|\n",
      "| .3434|       1|\n",
      "| .3890|       1|\n",
      "| /3685|       1|\n",
      "|     0|       6|\n",
      "|000078|    2784|\n",
      "| 00037|     132|\n",
      "|  0004|      53|\n",
      "| 00041|      35|\n",
      "| 00058|      11|\n",
      "|  0007|       4|\n",
      "| 00076|      11|\n",
      "| 00081|     140|\n",
      "| 00083|       2|\n",
      "|000861|       1|\n",
      "|  0009|       7|\n",
      "| 00091|    2742|\n",
      "+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_insnum_agg_1 = simple_groupby(spark, df_i94, 'insnum')\n",
    "df_insnum_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### airline\n",
    "\n",
    "AIRLINE - Airline used to arrive in U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|airline|records|\n",
      "+-------+-------+\n",
      "|   null|1308066|\n",
      "|    *FF|    952|\n",
      "|    *GA| 102156|\n",
      "|    *UU|    342|\n",
      "|    001|      6|\n",
      "|    006|     26|\n",
      "|    009|    101|\n",
      "|    00F|      1|\n",
      "|    00I|      2|\n",
      "|    00K|      1|\n",
      "|    00X|      8|\n",
      "|    012|      1|\n",
      "|    017|      8|\n",
      "|    018|     26|\n",
      "|    01B|      1|\n",
      "|    020|      1|\n",
      "|    026|    152|\n",
      "|    027|      2|\n",
      "|    029|     31|\n",
      "|    02Z|      2|\n",
      "+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airline_agg_1 = simple_groupby(spark, df_i94, 'airline')\n",
    "df_airline_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### admnum\n",
    "\n",
    "ADMNUM - Admission Number\n",
    "\n",
    "Note that the fact we have 498 records with admnum ID 0, shows that this is not currently a reliable globally unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|   admnum|records|\n",
      "+---------+-------+\n",
      "|      0.0|    498|\n",
      "|     27.0|      1|\n",
      "|     33.0|      1|\n",
      "|  79733.0|      1|\n",
      "| 197185.0|      1|\n",
      "| 452033.0|      1|\n",
      "| 971885.0|      1|\n",
      "|1004233.0|      1|\n",
      "|1078085.0|      1|\n",
      "|1218224.0|      1|\n",
      "|1219024.0|      1|\n",
      "|1219124.0|      1|\n",
      "|1219224.0|      1|\n",
      "|1219324.0|      1|\n",
      "|1219424.0|      1|\n",
      "|1222424.0|      1|\n",
      "|1226124.0|      1|\n",
      "|1226224.0|      1|\n",
      "|1233085.0|      1|\n",
      "|1236624.0|      1|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_admnum_agg_1 = simple_groupby(spark, df_i94, 'admnum')\n",
    "df_admnum_agg_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|min_admnum|    max_admnum|\n",
      "+----------+--------------+\n",
      "|       0.0|9.999999813E10|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    min(admnum) as min_admnum,\n",
    "    max(admnum) as max_admnum\n",
    "from df_i94\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### fltno\n",
    "\n",
    "FLTNO - Flight number of Airline used to arrive in U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|fltno|records|\n",
      "+-----+-------+\n",
      "| null| 333922|\n",
      "|    -|      1|\n",
      "|-PF10|      3|\n",
      "|-XCHI|      2|\n",
      "|-XCNP|      6|\n",
      "|-XCPF|     41|\n",
      "|0 111|      2|\n",
      "|0 178|      1|\n",
      "|0 188|      2|\n",
      "|0 294|      1|\n",
      "|0 301|      2|\n",
      "|0 314|      1|\n",
      "|0 336|      1|\n",
      "|0 608|      1|\n",
      "|0 612|      1|\n",
      "|0 622|      1|\n",
      "|0 874|      2|\n",
      "|0 941|      1|\n",
      "|   00|      2|\n",
      "|00 20|      1|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fltno_agg_1 = simple_groupby(spark, df_i94, 'fltno')\n",
    "df_fltno_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### visatype\n",
    "\n",
    "VISATYPE - Class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|visatype| records|\n",
      "+--------+--------+\n",
      "|      B1| 2282096|\n",
      "|      B2|15188785|\n",
      "|      CP|  272007|\n",
      "|     CPL|     236|\n",
      "|      E1|   48905|\n",
      "|      E2|  259215|\n",
      "|      F1| 1487432|\n",
      "|      F2|   68866|\n",
      "|     GMB|    2728|\n",
      "|     GMT| 1265275|\n",
      "|       I|   39054|\n",
      "|      I1|    2825|\n",
      "|      M1|   16306|\n",
      "|      M2|     667|\n",
      "|     SBP|      61|\n",
      "|      WB| 2940456|\n",
      "|      WT|16915615|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visatype_agg_1 = simple_groupby(spark, df_i94, 'visatype')\n",
    "df_visatype_agg_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Write groupby stats into CSV files\n",
    "\n",
    "We've done a lot of group-by stats above. Let's cache them into CSV files (showing top 1000 rows). So that our analytical teams may quickly get a feel of the distinct values (and associated counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV: raw_input_data/i94_sample_groupby_stats_csv/\n"
     ]
    }
   ],
   "source": [
    "RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV = config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV')\n",
    "print(f\"RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV: {RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV}\")\n",
    "\n",
    "def write_groupby_stats_csv(\n",
    "        df_spark,\n",
    "        groupby_field_name,\n",
    "        out_filename,\n",
    "        n_rows=1000,\n",
    "        out_dir=RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV\n",
    "    ):\n",
    "    \"\"\" given one spark dataframe (in a specific schema) save into a CSV file \"\"\"\n",
    "    \n",
    "    print(f'writing: {out_filename}')\n",
    "    pd.DataFrame(df_spark.take(n_rows), columns=[groupby_field_name, 'count'])\\\n",
    "        .to_csv(f\"{out_dir}/{out_filename}\", index=False)\n",
    "\n",
    "    \n",
    "\n",
    "def bulk_write_groupby_stats_csv(\n",
    "        n_rows=1000,\n",
    "        out_dir=RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV):\n",
    "    \"\"\" bulk run write_groupby_stats_csv \"\"\"\n",
    "    for e in [\n",
    "        (df_cicid_agg_1, 'cicid', \"df_cicid_agg_1.csv\"),\n",
    "        (df_i94yr_agg_1, 'i94yr', \"df_i94yr_agg_1.csv\"),\n",
    "        (df_i94mon_agg_1, 'i94mon', \"df_i94mon_agg_1.csv\"),\n",
    "        (df_i94cit_agg_1, 'i94cit', \"df_i94cit_agg_1.csv\"),\n",
    "        (df_i94res_agg_1, 'i94res', \"df_i94res_agg_1.csv\"),        \n",
    "        (df_i94port_agg_1, 'i94port', \"df_i94port_agg_1.csv\"),\n",
    "        (df_arrdate_agg_1, 'arrdate', \"df_arrdate_agg_1.csv\"),\n",
    "        (df_i94mode_agg_1, 'i94mode', \"df_i94mode_agg_1.csv\"),\n",
    "        (df_i94addr_agg_1, 'i94addr', \"df_i94addr_agg_1.csv\"),\n",
    "        (df_depdate_agg_1, 'depdate', \"df_depdate_agg_1.csv\"),\n",
    "        (df_i94bir_agg_1, 'i94bir', \"df_i94bir_agg_1.csv\"),\n",
    "        (df_i94visa_agg_1, 'i94visa', \"df_i94visa_agg_1.csv\"),\n",
    "        (df_count_agg_1, 'count', \"df_count_agg_1.csv\"),\n",
    "        (df_dtadfile_agg_1, 'dtadfile', \"df_dtadfile_agg_1.csv\"),\n",
    "        (df_visapost_agg_1, 'visapost', \"df_visapost_agg_1.csv\"),\n",
    "        (df_occup_agg_1, 'occup', \"df_occup_agg_1.csv\"),\n",
    "        (df_entdepa_agg_1, 'entdepa', \"df_entdepa_agg_1.csv\"),\n",
    "        (df_entdepd_agg_1, 'entdepd', \"df_entdepd_agg_1.csv\"),\n",
    "        (df_entdepu_agg_1, 'entdepu', \"df_entdepu_agg_1.csv\"),\n",
    "        (df_matflag_agg_1, 'matflag', \"df_matflag_agg_1.csv\"),        \n",
    "        (df_biryear_agg_1, 'biryear', \"df_biryear_agg_1.csv\"),   \n",
    "        (df_dtaddto_agg_1, 'dtaddto', \"df_dtaddto_agg_1.csv\"),   \n",
    "        (df_gender_agg_1, 'gender', \"df_gender_agg_1.csv\"),   \n",
    "        (df_insnum_agg_1, 'insnum', \"df_insnum_agg_1.csv\"),   \n",
    "        (df_airline_agg_1, 'airline', \"df_airline_agg_1.csv\"),   \n",
    "        (df_admnum_agg_1, 'admnum', \"df_admnum_agg_1.csv\"),   \n",
    "        (df_fltno_agg_1, 'fltno', \"df_fltno_agg_1.csv\"),\n",
    "        (df_visatype_agg_1, 'visatype', \"df_visatype_agg_1.csv\")\n",
    "    ]:\n",
    "        # write Spark DataFrame to a CSV file\n",
    "        write_groupby_stats_csv(\n",
    "            df_spark=e[0],\n",
    "            groupby_field_name=e[1],\n",
    "            out_filename=e[2],\n",
    "            n_rows=n_rows,\n",
    "            out_dir=out_dir,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing: df_cicid_agg_1.csv\n",
      "writing: df_i94yr_agg_1.csv\n",
      "writing: df_i94mon_agg_1.csv\n",
      "writing: df_i94cit_agg_1.csv\n",
      "writing: df_i94res_agg_1.csv\n",
      "writing: df_i94port_agg_1.csv\n",
      "writing: df_arrdate_agg_1.csv\n",
      "writing: df_i94mode_agg_1.csv\n",
      "writing: df_i94addr_agg_1.csv\n",
      "writing: df_depdate_agg_1.csv\n",
      "writing: df_i94bir_agg_1.csv\n",
      "writing: df_i94visa_agg_1.csv\n",
      "writing: df_count_agg_1.csv\n",
      "writing: df_dtadfile_agg_1.csv\n",
      "writing: df_visapost_agg_1.csv\n",
      "writing: df_occup_agg_1.csv\n",
      "writing: df_entdepa_agg_1.csv\n",
      "writing: df_entdepd_agg_1.csv\n",
      "writing: df_entdepu_agg_1.csv\n",
      "writing: df_matflag_agg_1.csv\n",
      "writing: df_biryear_agg_1.csv\n",
      "writing: df_dtaddto_agg_1.csv\n",
      "writing: df_gender_agg_1.csv\n",
      "writing: df_insnum_agg_1.csv\n",
      "writing: df_airline_agg_1.csv\n",
      "writing: df_admnum_agg_1.csv\n",
      "writing: df_fltno_agg_1.csv\n",
      "writing: df_visatype_agg_1.csv\n"
     ]
    }
   ],
   "source": [
    "bulk_write_groupby_stats_csv(n_rows=1000, out_dir=RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Assess Source 2: A SAS Program file that contains the lookup of codes to categorical string values\n",
    "\n",
    "Source 2: A SAS Program file that contains the lookup of codes to categorical string values\n",
    "\n",
    "* File format: `.SAS` Text File (containing SAS Code Syntax)\n",
    "* Content: this is a semi-structured text file that contains lookup logic from (numeric or character) codes to categorical string values, for the i94 Immigration Dataset columns: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`.\n",
    "* Remark: we will have an automated Python script to read in this text file, and auto derive CSV lookup tables (as descibed in bullet point 4 below).    \n",
    "* Official source location (no documentations identified. Likely come from the same source as source (1) above.)\n",
    "* Relative Path: `raw_input_data/i94_proc_format_code_sas/I94_SAS_Labels_Descriptions.SAS`\n",
    "\n",
    "Overall steps:\n",
    "\n",
    "1. Manually have a read of `I94_SAS_Labels_Descriptions.SAS`, try and get an idea of the lookup logic between categorical code and string values. Can we programmatically extract those mapping?\n",
    "2. Given the `I94_SAS_Labels_Descriptions.SAS` text file, programmatically create CSV lookup tables. Given the small dataset, we can do this with just Pandas (no Spark required).\n",
    "\n",
    "* lookup_i94cntyl.csv\n",
    "* lookup_i94port.csv\n",
    "* lookup_i94mode.csv\n",
    "* lookup_i94addr.csv\n",
    "* lookup_i94visa.csv\n",
    "\n",
    "3. Have a sanity check at these CSV files (e.g. ensure keys are unique)\n",
    "\n",
    "We have done all of the above via a seperate notebook already: `EDA_004_local_extract_i94_lookup_from_sas.ipynb`, but we will include in here for completeness (with enhancement to make turning into a proper Python scripts in future easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_I94_SAS_CODE_FILE: raw_input_data/i94_proc_format_code_sas/I94_SAS_Labels_Descriptions.SAS\n",
      "RAW_I94_LOOKUP_CSV_DIR: raw_input_data/i94_lookup_csv/\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94ADDR: raw_input_data/i94_lookup_csv/lookup_i94addr.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94CNTYL: raw_input_data/i94_lookup_csv/lookup_i94cntyl.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94PORT: raw_input_data/i94_lookup_csv/lookup_i94port.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94MODE: raw_input_data/i94_lookup_csv/lookup_i94mode.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94VISA: raw_input_data/i94_lookup_csv/lookup_i94visa.csv\n"
     ]
    }
   ],
   "source": [
    "# This utility aims to disect the semi-structured I94_SAS_Labels_Descriptions.SAS file\n",
    "# And return the lookup logic into tabular Pandas DataFrames and/or CSV files\n",
    "# For these i94 fields: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`\n",
    "# Note: the lookup for `i94cit` and `i94res` are the same. We group these into `i94cntyl` (country lookup)\n",
    "\n",
    "RAW_I94_SAS_CODE_FILE = config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_SAS_CODE_FILE')\n",
    "RAW_I94_LOOKUP_CSV_DIR = config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_DIR')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94ADDR=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94ADDR')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94CNTYL=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94CNTYL')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94PORT=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94PORT')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94MODE=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94MODE')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94VISA=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94VISA')\n",
    "\n",
    "print(f\"RAW_I94_SAS_CODE_FILE: {RAW_I94_SAS_CODE_FILE}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_DIR: {RAW_I94_LOOKUP_CSV_DIR}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94ADDR: {RAW_I94_LOOKUP_CSV_FILE_I94ADDR}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94CNTYL: {RAW_I94_LOOKUP_CSV_FILE_I94CNTYL}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94PORT: {RAW_I94_LOOKUP_CSV_FILE_I94PORT}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94MODE: {RAW_I94_LOOKUP_CSV_FILE_I94MODE}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94VISA: {RAW_I94_LOOKUP_CSV_FILE_I94VISA}\")\n",
    "\n",
    "def txt_to_list(txt_path):\n",
    "    \"\"\" Given a path to a text-like file, return a numpy array with all the text lines. \"\"\"\n",
    "    with open(txt_path) as f:\n",
    "        lines = f.readlines()     \n",
    "    return np.asarray(lines)\n",
    "\n",
    "\n",
    "def subset_lookup_lines(\n",
    "    np_array,\n",
    "    lookup_title: str,\n",
    "    start_str: str,\n",
    "    start_pos_offset: int,\n",
    "    end_str: str,\n",
    "    end_pos_offset: int):\n",
    "    \"\"\" Given a numpy array of text lines, return a dictionary that includes a the desirable\n",
    "    subset from the array (based on features) along with meta data. \n",
    "    Our objective is to pull out all the code to string mapping portion from the SAS code.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_pos, end_pos = None, None\n",
    "    for idx, line in enumerate(np_array):\n",
    "        #print(line)\n",
    "        if line == start_str:\n",
    "            start_pos = idx + start_pos_offset   \n",
    "            break\n",
    "            \n",
    "    for idx, line in enumerate(np_array[start_pos:]):            \n",
    "        if line == end_str:\n",
    "            end_pos = start_pos + idx + end_pos_offset\n",
    "            break\n",
    "        \n",
    "    return {\n",
    "        \"lookup_title\": lookup_title,\n",
    "        \"start_pos\": start_pos,\n",
    "        \"end_pos\": end_pos,\n",
    "        \"subset\": np_array[start_pos:end_pos].copy()\n",
    "    }\n",
    "\n",
    "\n",
    "def array_to_df(np_array, numeric_key=True, sort_by='key'):\n",
    "    \"\"\" Given a numpy array of text lines that contains the semi-structured SAS PROC FORMAT Value statements\n",
    "    (that contains lookup of categorial codes to texts), return a structured tabular Pandas DataFrame )\n",
    "    Our objective here is to obtain a tabular code to string mapping portion from the PROC FORMAT VALUE statements\n",
    "    (from upstream SAS code)\n",
    "    \"\"\"\n",
    "    lookup_list = []\n",
    "    for idx, line in enumerate(np_array):\n",
    "        left_right = line.split(\"=\")\n",
    "        left, right = left_right[0], left_right[1]\n",
    "        if numeric_key:\n",
    "            key = int(left.strip())\n",
    "        else:\n",
    "            key = left.strip().replace(\"'\", \"\").strip()\n",
    "        value = right.strip().replace(\"'\", \"\").replace(\";\", \"\").replace(\"\\n\", \"\").strip()\n",
    "        lookup_list.append({\n",
    "            \"key\": key,\n",
    "            \"value\": value\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(lookup_list).sort_values(by=sort_by)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_i94_lookup_dataframes(\n",
    "        np_array,\n",
    "        lookup_title: str,\n",
    "        start_str: str,\n",
    "        start_pos_offset: int,\n",
    "        end_str: str,\n",
    "        end_pos_offset: int,\n",
    "        numeric_key: bool = False\n",
    "    ):\n",
    "    \"\"\" Given a numpy array with SAS code proc format value statement lines,\n",
    "    return a Pandas DataFrame with lookup key and string value \"\"\"\n",
    "    \n",
    "    df_pd = array_to_df(\n",
    "        subset_lookup_lines(\n",
    "            np_array,\n",
    "            lookup_title,\n",
    "            start_str,\n",
    "            start_pos_offset,\n",
    "            end_str,\n",
    "            end_pos_offset\n",
    "        )[\"subset\"],\n",
    "        numeric_key=False,\n",
    "        sort_by='key'\n",
    "    )\n",
    "    return df_pd\n",
    "\n",
    "\n",
    "def bulk_build_i94_dataframes(input_file: str = RAW_I94_SAS_CODE_FILE) -> dict:\n",
    "    \"\"\" Given I94_SAS_Labels_Descriptions.SAS, return all the lookup dataframes  \"\"\"\n",
    "    \n",
    "    lines = txt_to_list('raw_input_data/i94_proc_format_code_sas/I94_SAS_Labels_Descriptions.SAS')\n",
    "    \n",
    "    print(f\"Building i94 lookup dataframes from text file: {input_file}\")\n",
    "    df_i94addr = build_i94_lookup_dataframes(lines, 'i94addr', 'value i94addrl\\n', 1, '\\n', 0)\n",
    "    df_i94cntyl = build_i94_lookup_dataframes(lines, 'i94cntyl', '  value i94cntyl\\n', 1, '\\n', 0)\n",
    "    df_i94port = build_i94_lookup_dataframes(lines, 'i94port', '  value $i94prtl\\n', 1, ';\\n', 0)\n",
    "    df_i94mode = build_i94_lookup_dataframes(lines, 'i94mode', 'value i94model\\n', 1, '\\t\\n', 0)\n",
    "    df_i94visa = build_i94_lookup_dataframes(lines, 'i94visa', '/* I94VISA - Visa codes collapsed into three categories:\\n', 1, '*/\\n', 0)\n",
    "    print(\"Dataframe build Done.\")\n",
    "    \n",
    "    return {\n",
    "        \"i94addr\": df_i94addr,\n",
    "        \"i94cntyl\": df_i94cntyl,\n",
    "        \"i94mode\": df_i94mode,       \n",
    "        \"i94port\": df_i94port,\n",
    "        \"i94visa\": df_i94visa,\n",
    "    }\n",
    "\n",
    "\n",
    "def bulk_export_i94_csv_files(\n",
    "        input_file: str = RAW_I94_SAS_CODE_FILE,\n",
    "        output_dir: str = RAW_I94_LOOKUP_CSV_DIR,\n",
    "        output_file_i94addr: str = RAW_I94_LOOKUP_CSV_FILE_I94ADDR,\n",
    "        output_file_i94cntyl: str = RAW_I94_LOOKUP_CSV_FILE_I94CNTYL,\n",
    "        output_file_i94port: str = RAW_I94_LOOKUP_CSV_FILE_I94PORT,\n",
    "        output_file_i94mode: str = RAW_I94_LOOKUP_CSV_FILE_I94MODE,\n",
    "        output_file_i94visa: str = RAW_I94_LOOKUP_CSV_FILE_I94VISA,\n",
    "    ):\n",
    "    \"\"\" Given I94_SAS_Labels_Descriptions.SAS, generate all the lookup CSV files \"\"\"\n",
    "    \n",
    "    dataframes_to_export = bulk_build_i94_dataframes(input_file=RAW_I94_SAS_CODE_FILE)\n",
    "    \n",
    "    print(f\"Exporting i94 lookup CSV files to: {output_dir}\")\n",
    "    dataframes_to_export[\"i94addr\"].to_csv(output_file_i94addr, sep=',', index=False)\n",
    "    dataframes_to_export[\"i94cntyl\"].to_csv(output_file_i94cntyl, sep=',', index=False)\n",
    "    dataframes_to_export[\"i94port\"].to_csv(output_file_i94port, sep=',', index=False)\n",
    "    dataframes_to_export[\"i94mode\"].to_csv(output_file_i94mode, sep=',', index=False)\n",
    "    dataframes_to_export[\"i94visa\"].to_csv(output_file_i94visa, sep=',', index=False)\n",
    "    print(\"CSV Export Done.\")\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# bulk_build_i94_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building i94 lookup dataframes from text file: raw_input_data/i94_proc_format_code_sas/I94_SAS_Labels_Descriptions.SAS\n",
      "Dataframe build Done.\n",
      "Exporting i94 lookup CSV files to: raw_input_data/i94_lookup_csv/\n",
      "CSV Export Done.\n"
     ]
    }
   ],
   "source": [
    "bulk_export_i94_csv_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Sanity Data Quality check the lookup CSV Files\n",
    "\n",
    "This is our Mini Data Quality Check. At a minimum the lookup table `key` column must be unique and non-null.\n",
    "\n",
    "Conclusion: all 5 lookup CSV files have unique and non-null keys. (passed all the assert tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quality_check_lookup_csv_files(input_path, column_to_check, delimiter=','):\n",
    "    \"\"\" Given an input CSV file, assert a column is unique and non null \"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    total_rows = len(df[column_to_check])\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(df.head(5))\n",
    "    if len(df) >= 10:\n",
    "        print(df.tail())\n",
    "    # Ensure all key values are unique\n",
    "    assert len(df[column_to_check]) == len(df[column_to_check].unique())\n",
    "    # Ensure there are no null (None) values in the key column\n",
    "    assert df[column_to_check].isnull().values.any() == False\n",
    "    print(\"All keys are unique and non-null.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 55\n",
      "  key            value\n",
      "0  99  All Other Codes\n",
      "1  AK           ALASKA\n",
      "2  AL          ALABAMA\n",
      "3  AR         ARKANSAS\n",
      "4  AZ          ARIZONA\n",
      "   key        value\n",
      "50  VT      VERMONT\n",
      "51  WA   WASHINGTON\n",
      "52  WI    WISCONSON\n",
      "53  WV  W. VIRGINIA\n",
      "54  WY      WYOMING\n",
      "All keys are unique and non-null.\n"
     ]
    }
   ],
   "source": [
    "quality_check_lookup_csv_files(RAW_I94_LOOKUP_CSV_FILE_I94ADDR, \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 289\n",
      "   key                  value\n",
      "0    0     INVALID: STATELESS\n",
      "1  100  No Country Code (100)\n",
      "2  101                ALBANIA\n",
      "3  102                ANDORRA\n",
      "4  103                AUSTRIA\n",
      "     key                  value\n",
      "284  849  No Country Code (849)\n",
      "285  914  No Country Code (914)\n",
      "286  944  No Country Code (944)\n",
      "287  996  No Country Code (996)\n",
      "288  999       INVALID: UNKNOWN\n",
      "All keys are unique and non-null.\n"
     ]
    }
   ],
   "source": [
    "quality_check_lookup_csv_files(RAW_I94_LOOKUP_CSV_FILE_I94CNTYL, \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 660\n",
      "   key                      value\n",
      "0  .GA         No PORT Code (.GA)\n",
      "1  060          No PORT Code (60)\n",
      "2  48Y  PINECREEK BORDER ARPT, MN\n",
      "3  5KE              KETCHIKAN, AK\n",
      "4  5T6         No PORT Code (5T6)\n",
      "     key                          value\n",
      "655  YSL                     YSLETA, TX\n",
      "656  YUI                       YUMA, AZ\n",
      "657  YUM                       YUMA, AZ\n",
      "658  YXE              SASKATOON, CANADA\n",
      "659  ZZZ  MEXICO Land (Banco de Mexico)\n",
      "All keys are unique and non-null.\n"
     ]
    }
   ],
   "source": [
    "quality_check_lookup_csv_files(RAW_I94_LOOKUP_CSV_FILE_I94PORT, \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4\n",
      "   key         value\n",
      "0    1           Air\n",
      "1    2           Sea\n",
      "2    3          Land\n",
      "3    9  Not reported\n",
      "All keys are unique and non-null.\n"
     ]
    }
   ],
   "source": [
    "quality_check_lookup_csv_files(RAW_I94_LOOKUP_CSV_FILE_I94MODE, \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 3\n",
      "   key     value\n",
      "0    1  Business\n",
      "1    2  Pleasure\n",
      "2    3   Student\n",
      "All keys are unique and non-null.\n"
     ]
    }
   ],
   "source": [
    "quality_check_lookup_csv_files(RAW_I94_LOOKUP_CSV_FILE_I94VISA, \"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Assess Source 3: US Airport Codes\n",
    "\n",
    "(Due to time limitation we have decided to only use Source 1, 2, and 4 in our eventual data model. We will take a peek at this source 3 nevertheless in case we wish to revisit at a later time.)\n",
    "\n",
    "* File format: `.CSV` File (Delimiter: comma `,`)\n",
    "* Content: contains the airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia).\n",
    "* Remark: we do not use this dataset yet. But we may in future.\n",
    "* Official source location: [Airport Codes data by datahub.io](https://datahub.io/core/airport-codes#data)\n",
    "* Relative path: `raw_input_data/airport_codes_csv/airport-codes_csv.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_AIRPORT_CODES: raw_input_data/airport_codes_csv/airport-codes_csv.csv\n"
     ]
    }
   ],
   "source": [
    "RAW_AIRPORT_CODES=config_dev.get('DATA_PATHS_LOCAL', 'RAW_AIRPORT_CODES')\n",
    "print(f\"RAW_AIRPORT_CODES: {RAW_AIRPORT_CODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00AS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fulton Airport</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-OK</td>\n",
       "      <td>Alex</td>\n",
       "      <td>00AS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AS</td>\n",
       "      <td>-97.8180194, 34.9428028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00AZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Cordes Airport</td>\n",
       "      <td>3810.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>Cordes</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>-112.16500091552734, 34.305599212646484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00CA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goldstone /Gts/ Airport</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Barstow</td>\n",
       "      <td>00CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CA</td>\n",
       "      <td>-116.888000488, 35.350498199499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00CL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Williams Ag Airport</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Biggs</td>\n",
       "      <td>00CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CL</td>\n",
       "      <td>-121.763427, 39.427188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00CN</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kitchen Creek Helibase Heliport</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Pine Valley</td>\n",
       "      <td>00CN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CN</td>\n",
       "      <td>-116.4597417, 32.7273736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00CO</td>\n",
       "      <td>closed</td>\n",
       "      <td>Cass Field</td>\n",
       "      <td>4830.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CO</td>\n",
       "      <td>Briggsdale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-104.344002, 40.622202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00FA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Grass Patch Airport</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Bushnell</td>\n",
       "      <td>00FA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00FA</td>\n",
       "      <td>-82.21900177001953, 28.64550018310547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00FD</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Ringhaver Heliport</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Riverview</td>\n",
       "      <td>00FD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00FD</td>\n",
       "      <td>-82.34539794921875, 28.846599578857422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00FL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>River Oak Airport</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Okeechobee</td>\n",
       "      <td>00FL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00FL</td>\n",
       "      <td>-80.96920013427734, 27.230899810791016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00GA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lt World Airport</td>\n",
       "      <td>700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-GA</td>\n",
       "      <td>Lithonia</td>\n",
       "      <td>00GA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GA</td>\n",
       "      <td>-84.06829833984375, 33.76750183105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00GE</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Caffrey Heliport</td>\n",
       "      <td>957.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-GA</td>\n",
       "      <td>Hiram</td>\n",
       "      <td>00GE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GE</td>\n",
       "      <td>-84.73390197753906, 33.88420104980469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00HI</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kaupulehu Heliport</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-HI</td>\n",
       "      <td>Kailua/Kona</td>\n",
       "      <td>00HI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00HI</td>\n",
       "      <td>-155.980233, 19.832715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00ID</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Delta Shores Airport</td>\n",
       "      <td>2064.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-ID</td>\n",
       "      <td>Clark Fork</td>\n",
       "      <td>00ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00ID</td>\n",
       "      <td>-116.21399688720703, 48.145301818847656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00IG</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goltl Airport</td>\n",
       "      <td>3359.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>McDonald</td>\n",
       "      <td>00IG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00IG</td>\n",
       "      <td>-101.395994, 39.724028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00II</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Bailey Generation Station Heliport</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-IN</td>\n",
       "      <td>Chesterton</td>\n",
       "      <td>00II</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00II</td>\n",
       "      <td>-87.122802734375, 41.644500732421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>00IL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Hammer Airport</td>\n",
       "      <td>840.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-IL</td>\n",
       "      <td>Polo</td>\n",
       "      <td>00IL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00IL</td>\n",
       "      <td>-89.5604019165039, 41.97840118408203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00IN</td>\n",
       "      <td>heliport</td>\n",
       "      <td>St Mary Medical Center Heliport</td>\n",
       "      <td>634.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-IN</td>\n",
       "      <td>Hobart</td>\n",
       "      <td>00IN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00IN</td>\n",
       "      <td>-87.2605972290039, 41.51139831542969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>00IS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Hayenga's Cant Find Farms Airport</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-IL</td>\n",
       "      <td>Kings</td>\n",
       "      <td>00IS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00IS</td>\n",
       "      <td>-89.1229019165039, 40.02560043334961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>00KS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Hayden Farm Airport</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Gardner</td>\n",
       "      <td>00KS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00KS</td>\n",
       "      <td>-94.93049621582031, 38.72779846191406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>00KY</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Robbins Roost Airport</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KY</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>00KY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00KY</td>\n",
       "      <td>-84.61969757080078, 37.409400939941406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>00LA</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Shell Chemical East Site Heliport</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-LA</td>\n",
       "      <td>Gonzales</td>\n",
       "      <td>00LA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00LA</td>\n",
       "      <td>-90.980833, 30.191944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>00LL</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Ac &amp; R Components Heliport</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-IL</td>\n",
       "      <td>Chatham</td>\n",
       "      <td>00LL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00LL</td>\n",
       "      <td>-89.70559692382812, 39.66529846191406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>00LS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lejeune Airport</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-LA</td>\n",
       "      <td>Esterwood</td>\n",
       "      <td>00LS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00LS</td>\n",
       "      <td>-92.42939758300781, 30.13629913330078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>00MD</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Slater Field</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-MD</td>\n",
       "      <td>Federalsburg</td>\n",
       "      <td>00MD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00MD</td>\n",
       "      <td>-75.75379943847656, 38.75709915161133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>00MI</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Dow Chemical Heliport</td>\n",
       "      <td>588.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-MI</td>\n",
       "      <td>Ludington</td>\n",
       "      <td>00MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00MI</td>\n",
       "      <td>-86.41670227050781, 43.94940185546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55045</th>\n",
       "      <td>ZYAS</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Anshan Air Base</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Anshan</td>\n",
       "      <td>ZYAS</td>\n",
       "      <td>AOG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.853996, 41.105301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55046</th>\n",
       "      <td>ZYCC</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Longjia Airport</td>\n",
       "      <td>706.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-22</td>\n",
       "      <td>Changchun</td>\n",
       "      <td>ZYCC</td>\n",
       "      <td>CGQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.684997559, 43.9962005615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55047</th>\n",
       "      <td>ZYCH</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Changhai Airport</td>\n",
       "      <td>80.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Changhai</td>\n",
       "      <td>ZYCH</td>\n",
       "      <td>CNI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.666944444, 39.2666666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55048</th>\n",
       "      <td>ZYCY</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Chaoyang Airport</td>\n",
       "      <td>568.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Chaoyang</td>\n",
       "      <td>ZYCY</td>\n",
       "      <td>CHG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.434998, 41.538101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55049</th>\n",
       "      <td>ZYDU</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Wudalianchi Dedu Airport</td>\n",
       "      <td>984.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Wudalianchi</td>\n",
       "      <td>ZYDU</td>\n",
       "      <td>DTU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.133, 48.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55050</th>\n",
       "      <td>ZYFX</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fuxin Airport</td>\n",
       "      <td>548.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Fuxin</td>\n",
       "      <td>ZYFX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.718122, 42.069014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55051</th>\n",
       "      <td>ZYFY</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Dongji Aiport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Fuyuan</td>\n",
       "      <td>ZYFY</td>\n",
       "      <td>FYJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.366447, 48.199494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55052</th>\n",
       "      <td>ZYHB</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>Taiping Airport</td>\n",
       "      <td>457.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Harbin</td>\n",
       "      <td>ZYHB</td>\n",
       "      <td>HRB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.25, 45.6234016418457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55053</th>\n",
       "      <td>ZYHE</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Heihe Airport</td>\n",
       "      <td>8530.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Heihe</td>\n",
       "      <td>ZYHE</td>\n",
       "      <td>HEK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.308883667, 50.1716209371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55054</th>\n",
       "      <td>ZYJL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Jilin Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-22</td>\n",
       "      <td>Jilin</td>\n",
       "      <td>ZYJL</td>\n",
       "      <td>JIL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.396004, 44.002201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55055</th>\n",
       "      <td>ZYJM</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Jiamusi Airport</td>\n",
       "      <td>262.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Jiamusi</td>\n",
       "      <td>ZYJM</td>\n",
       "      <td>JMU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.464996338, 46.84339904789999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55056</th>\n",
       "      <td>ZYJS</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Jiansanjiang Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Jiansanjiang</td>\n",
       "      <td>ZYJS</td>\n",
       "      <td>JSJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.660278, 47.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55057</th>\n",
       "      <td>ZYJX</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Jixi Xingkaihu Airport</td>\n",
       "      <td>760.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Jixi</td>\n",
       "      <td>ZYJX</td>\n",
       "      <td>JXA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.193, 45.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55058</th>\n",
       "      <td>ZYJZ</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Jinzhou Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Jinzhou</td>\n",
       "      <td>ZYJZ</td>\n",
       "      <td>JNZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.06199645996094, 41.10139846801758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55059</th>\n",
       "      <td>ZYLD</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Lindu Airport</td>\n",
       "      <td>791.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Yichun</td>\n",
       "      <td>ZYLD</td>\n",
       "      <td>LDS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.019125, 47.7520555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55060</th>\n",
       "      <td>ZYLS</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Yushu Batang Airport</td>\n",
       "      <td>12816.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-63</td>\n",
       "      <td>Yushu</td>\n",
       "      <td>ZYLS</td>\n",
       "      <td>YUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0363888889, 32.836388888900004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55061</th>\n",
       "      <td>ZYMD</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Mudanjiang Hailang International Airport</td>\n",
       "      <td>883.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Mudanjiang</td>\n",
       "      <td>ZYMD</td>\n",
       "      <td>MDG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.569000244, 44.5241012573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55062</th>\n",
       "      <td>ZYMH</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Gu-Lian Airport</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Mohe</td>\n",
       "      <td>ZYMH</td>\n",
       "      <td>OHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.43, 52.912777777799995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55063</th>\n",
       "      <td>ZYQQ</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Qiqihar Sanjiazi Airport</td>\n",
       "      <td>477.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Qiqihar</td>\n",
       "      <td>ZYQQ</td>\n",
       "      <td>NDG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.91799926757812, 47.239601135253906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55064</th>\n",
       "      <td>ZYSQ</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Songyuan Chaganhu Airport</td>\n",
       "      <td>459.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-22</td>\n",
       "      <td>Qian Gorlos Mongol Autonomous County</td>\n",
       "      <td>ZYSQ</td>\n",
       "      <td>YSQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.550178, 44.938114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55065</th>\n",
       "      <td>ZYTH</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Tahe Airport</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-23</td>\n",
       "      <td>Tahe</td>\n",
       "      <td>ZYTH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.720222222, 52.2244444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55066</th>\n",
       "      <td>ZYTL</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>Zhoushuizi Airport</td>\n",
       "      <td>107.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Dalian</td>\n",
       "      <td>ZYTL</td>\n",
       "      <td>DLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.53900146484375, 38.9656982421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55067</th>\n",
       "      <td>ZYTN</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Tonghua Sanyuanpu Airport</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-22</td>\n",
       "      <td>Tonghua</td>\n",
       "      <td>ZYTN</td>\n",
       "      <td>TNH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.703333333, 42.2538888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55068</th>\n",
       "      <td>ZYTX</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>Taoxian Airport</td>\n",
       "      <td>198.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Shenyang</td>\n",
       "      <td>ZYTX</td>\n",
       "      <td>SHE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.48300170898438, 41.639801025390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55069</th>\n",
       "      <td>ZYYJ</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Yanji Chaoyangchuan Airport</td>\n",
       "      <td>624.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-22</td>\n",
       "      <td>Yanji</td>\n",
       "      <td>ZYYJ</td>\n",
       "      <td>YNJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.451004028, 42.8828010559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55070</th>\n",
       "      <td>ZYYK</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Yingkou Lanqi Airport</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Yingkou</td>\n",
       "      <td>ZYYK</td>\n",
       "      <td>YKH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.3586, 40.542524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55071</th>\n",
       "      <td>ZYYY</td>\n",
       "      <td>medium_airport</td>\n",
       "      <td>Shenyang Dongta Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>CN-21</td>\n",
       "      <td>Shenyang</td>\n",
       "      <td>ZYYY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.49600219726562, 41.784400939941406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55072</th>\n",
       "      <td>ZZ-0001</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Sealand Helipad</td>\n",
       "      <td>40.0</td>\n",
       "      <td>EU</td>\n",
       "      <td>GB</td>\n",
       "      <td>GB-ENG</td>\n",
       "      <td>Sealand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4825, 51.894444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55073</th>\n",
       "      <td>ZZ-0002</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Glorioso Islands Airstrip</td>\n",
       "      <td>11.0</td>\n",
       "      <td>AF</td>\n",
       "      <td>TF</td>\n",
       "      <td>TF-U-A</td>\n",
       "      <td>Grande Glorieuse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.296388888900005, -11.584277777799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55074</th>\n",
       "      <td>ZZZZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Satsuma IÅjima Airport</td>\n",
       "      <td>338.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>JP</td>\n",
       "      <td>JP-46</td>\n",
       "      <td>Mishima-Mura</td>\n",
       "      <td>RJX7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.270556, 30.784722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55075 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ident            type                                      name  \\\n",
       "0          00A        heliport                         Total Rf Heliport   \n",
       "1         00AA   small_airport                      Aero B Ranch Airport   \n",
       "2         00AK   small_airport                              Lowell Field   \n",
       "3         00AL   small_airport                              Epps Airpark   \n",
       "4         00AR          closed        Newport Hospital & Clinic Heliport   \n",
       "5         00AS   small_airport                            Fulton Airport   \n",
       "6         00AZ   small_airport                            Cordes Airport   \n",
       "7         00CA   small_airport                   Goldstone /Gts/ Airport   \n",
       "8         00CL   small_airport                       Williams Ag Airport   \n",
       "9         00CN        heliport           Kitchen Creek Helibase Heliport   \n",
       "10        00CO          closed                                Cass Field   \n",
       "11        00FA   small_airport                       Grass Patch Airport   \n",
       "12        00FD        heliport                        Ringhaver Heliport   \n",
       "13        00FL   small_airport                         River Oak Airport   \n",
       "14        00GA   small_airport                          Lt World Airport   \n",
       "15        00GE        heliport                          Caffrey Heliport   \n",
       "16        00HI        heliport                        Kaupulehu Heliport   \n",
       "17        00ID   small_airport                      Delta Shores Airport   \n",
       "18        00IG   small_airport                             Goltl Airport   \n",
       "19        00II        heliport        Bailey Generation Station Heliport   \n",
       "20        00IL   small_airport                            Hammer Airport   \n",
       "21        00IN        heliport           St Mary Medical Center Heliport   \n",
       "22        00IS   small_airport         Hayenga's Cant Find Farms Airport   \n",
       "23        00KS   small_airport                       Hayden Farm Airport   \n",
       "24        00KY   small_airport                     Robbins Roost Airport   \n",
       "25        00LA        heliport         Shell Chemical East Site Heliport   \n",
       "26        00LL        heliport                Ac & R Components Heliport   \n",
       "27        00LS   small_airport                           Lejeune Airport   \n",
       "28        00MD   small_airport                              Slater Field   \n",
       "29        00MI        heliport                     Dow Chemical Heliport   \n",
       "...        ...             ...                                       ...   \n",
       "55045     ZYAS  medium_airport                           Anshan Air Base   \n",
       "55046     ZYCC  medium_airport                           Longjia Airport   \n",
       "55047     ZYCH  medium_airport                          Changhai Airport   \n",
       "55048     ZYCY  medium_airport                          Chaoyang Airport   \n",
       "55049     ZYDU  medium_airport                  Wudalianchi Dedu Airport   \n",
       "55050     ZYFX   small_airport                             Fuxin Airport   \n",
       "55051     ZYFY  medium_airport                             Dongji Aiport   \n",
       "55052     ZYHB   large_airport                           Taiping Airport   \n",
       "55053     ZYHE  medium_airport                             Heihe Airport   \n",
       "55054     ZYJL   small_airport                             Jilin Airport   \n",
       "55055     ZYJM  medium_airport                           Jiamusi Airport   \n",
       "55056     ZYJS  medium_airport                      Jiansanjiang Airport   \n",
       "55057     ZYJX  medium_airport                    Jixi Xingkaihu Airport   \n",
       "55058     ZYJZ  medium_airport                           Jinzhou Airport   \n",
       "55059     ZYLD  medium_airport                             Lindu Airport   \n",
       "55060     ZYLS  medium_airport                      Yushu Batang Airport   \n",
       "55061     ZYMD  medium_airport  Mudanjiang Hailang International Airport   \n",
       "55062     ZYMH  medium_airport                           Gu-Lian Airport   \n",
       "55063     ZYQQ  medium_airport                  Qiqihar Sanjiazi Airport   \n",
       "55064     ZYSQ  medium_airport                 Songyuan Chaganhu Airport   \n",
       "55065     ZYTH   small_airport                              Tahe Airport   \n",
       "55066     ZYTL   large_airport                        Zhoushuizi Airport   \n",
       "55067     ZYTN  medium_airport                 Tonghua Sanyuanpu Airport   \n",
       "55068     ZYTX   large_airport                           Taoxian Airport   \n",
       "55069     ZYYJ  medium_airport               Yanji Chaoyangchuan Airport   \n",
       "55070     ZYYK  medium_airport                     Yingkou Lanqi Airport   \n",
       "55071     ZYYY  medium_airport                   Shenyang Dongta Airport   \n",
       "55072  ZZ-0001        heliport                           Sealand Helipad   \n",
       "55073  ZZ-0002   small_airport                 Glorioso Islands Airstrip   \n",
       "55074     ZZZZ   small_airport                   Satsuma IÅjima Airport   \n",
       "\n",
       "       elevation_ft continent iso_country iso_region  \\\n",
       "0              11.0       NaN          US      US-PA   \n",
       "1            3435.0       NaN          US      US-KS   \n",
       "2             450.0       NaN          US      US-AK   \n",
       "3             820.0       NaN          US      US-AL   \n",
       "4             237.0       NaN          US      US-AR   \n",
       "5            1100.0       NaN          US      US-OK   \n",
       "6            3810.0       NaN          US      US-AZ   \n",
       "7            3038.0       NaN          US      US-CA   \n",
       "8              87.0       NaN          US      US-CA   \n",
       "9            3350.0       NaN          US      US-CA   \n",
       "10           4830.0       NaN          US      US-CO   \n",
       "11             53.0       NaN          US      US-FL   \n",
       "12             25.0       NaN          US      US-FL   \n",
       "13             35.0       NaN          US      US-FL   \n",
       "14            700.0       NaN          US      US-GA   \n",
       "15            957.0       NaN          US      US-GA   \n",
       "16             43.0       NaN          US      US-HI   \n",
       "17           2064.0       NaN          US      US-ID   \n",
       "18           3359.0       NaN          US      US-KS   \n",
       "19            600.0       NaN          US      US-IN   \n",
       "20            840.0       NaN          US      US-IL   \n",
       "21            634.0       NaN          US      US-IN   \n",
       "22            820.0       NaN          US      US-IL   \n",
       "23           1100.0       NaN          US      US-KS   \n",
       "24           1265.0       NaN          US      US-KY   \n",
       "25             15.0       NaN          US      US-LA   \n",
       "26            600.0       NaN          US      US-IL   \n",
       "27             12.0       NaN          US      US-LA   \n",
       "28             45.0       NaN          US      US-MD   \n",
       "29            588.0       NaN          US      US-MI   \n",
       "...             ...       ...         ...        ...   \n",
       "55045           NaN        AS          CN      CN-21   \n",
       "55046         706.0        AS          CN      CN-22   \n",
       "55047          80.0        AS          CN      CN-21   \n",
       "55048         568.0        AS          CN      CN-21   \n",
       "55049         984.0        AS          CN      CN-23   \n",
       "55050         548.0        AS          CN      CN-21   \n",
       "55051           NaN        AS          CN      CN-23   \n",
       "55052         457.0        AS          CN      CN-23   \n",
       "55053        8530.0        AS          CN      CN-23   \n",
       "55054           NaN        AS          CN      CN-22   \n",
       "55055         262.0        AS          CN      CN-23   \n",
       "55056           NaN        AS          CN      CN-23   \n",
       "55057         760.0        AS          CN      CN-23   \n",
       "55058           NaN        AS          CN      CN-21   \n",
       "55059         791.0        AS          CN      CN-23   \n",
       "55060       12816.0        AS          CN      CN-63   \n",
       "55061         883.0        AS          CN      CN-23   \n",
       "55062        1836.0        AS          CN      CN-23   \n",
       "55063         477.0        AS          CN      CN-23   \n",
       "55064         459.0        AS          CN      CN-22   \n",
       "55065        1240.0        AS          CN      CN-23   \n",
       "55066         107.0        AS          CN      CN-21   \n",
       "55067        1200.0        AS          CN      CN-22   \n",
       "55068         198.0        AS          CN      CN-21   \n",
       "55069         624.0        AS          CN      CN-22   \n",
       "55070           0.0        AS          CN      CN-21   \n",
       "55071           NaN        AS          CN      CN-21   \n",
       "55072          40.0        EU          GB     GB-ENG   \n",
       "55073          11.0        AF          TF     TF-U-A   \n",
       "55074         338.0        AS          JP      JP-46   \n",
       "\n",
       "                               municipality gps_code iata_code local_code  \\\n",
       "0                                  Bensalem      00A       NaN        00A   \n",
       "1                                     Leoti     00AA       NaN       00AA   \n",
       "2                              Anchor Point     00AK       NaN       00AK   \n",
       "3                                   Harvest     00AL       NaN       00AL   \n",
       "4                                   Newport      NaN       NaN        NaN   \n",
       "5                                      Alex     00AS       NaN       00AS   \n",
       "6                                    Cordes     00AZ       NaN       00AZ   \n",
       "7                                   Barstow     00CA       NaN       00CA   \n",
       "8                                     Biggs     00CL       NaN       00CL   \n",
       "9                               Pine Valley     00CN       NaN       00CN   \n",
       "10                               Briggsdale      NaN       NaN        NaN   \n",
       "11                                 Bushnell     00FA       NaN       00FA   \n",
       "12                                Riverview     00FD       NaN       00FD   \n",
       "13                               Okeechobee     00FL       NaN       00FL   \n",
       "14                                 Lithonia     00GA       NaN       00GA   \n",
       "15                                    Hiram     00GE       NaN       00GE   \n",
       "16                              Kailua/Kona     00HI       NaN       00HI   \n",
       "17                               Clark Fork     00ID       NaN       00ID   \n",
       "18                                 McDonald     00IG       NaN       00IG   \n",
       "19                               Chesterton     00II       NaN       00II   \n",
       "20                                     Polo     00IL       NaN       00IL   \n",
       "21                                   Hobart     00IN       NaN       00IN   \n",
       "22                                    Kings     00IS       NaN       00IS   \n",
       "23                                  Gardner     00KS       NaN       00KS   \n",
       "24                                 Stanford     00KY       NaN       00KY   \n",
       "25                                 Gonzales     00LA       NaN       00LA   \n",
       "26                                  Chatham     00LL       NaN       00LL   \n",
       "27                                Esterwood     00LS       NaN       00LS   \n",
       "28                             Federalsburg     00MD       NaN       00MD   \n",
       "29                                Ludington     00MI       NaN       00MI   \n",
       "...                                     ...      ...       ...        ...   \n",
       "55045                                Anshan     ZYAS       AOG        NaN   \n",
       "55046                             Changchun     ZYCC       CGQ        NaN   \n",
       "55047                              Changhai     ZYCH       CNI        NaN   \n",
       "55048                              Chaoyang     ZYCY       CHG        NaN   \n",
       "55049                           Wudalianchi     ZYDU       DTU        NaN   \n",
       "55050                                 Fuxin     ZYFX       NaN        NaN   \n",
       "55051                                Fuyuan     ZYFY       FYJ        NaN   \n",
       "55052                                Harbin     ZYHB       HRB        NaN   \n",
       "55053                                 Heihe     ZYHE       HEK        NaN   \n",
       "55054                                 Jilin     ZYJL       JIL        NaN   \n",
       "55055                               Jiamusi     ZYJM       JMU        NaN   \n",
       "55056                          Jiansanjiang     ZYJS       JSJ        NaN   \n",
       "55057                                  Jixi     ZYJX       JXA        NaN   \n",
       "55058                               Jinzhou     ZYJZ       JNZ        NaN   \n",
       "55059                                Yichun     ZYLD       LDS        NaN   \n",
       "55060                                 Yushu     ZYLS       YUS        NaN   \n",
       "55061                            Mudanjiang     ZYMD       MDG        NaN   \n",
       "55062                                  Mohe     ZYMH       OHE        NaN   \n",
       "55063                               Qiqihar     ZYQQ       NDG        NaN   \n",
       "55064  Qian Gorlos Mongol Autonomous County     ZYSQ       YSQ        NaN   \n",
       "55065                                  Tahe     ZYTH       NaN        NaN   \n",
       "55066                                Dalian     ZYTL       DLC        NaN   \n",
       "55067                               Tonghua     ZYTN       TNH        NaN   \n",
       "55068                              Shenyang     ZYTX       SHE        NaN   \n",
       "55069                                 Yanji     ZYYJ       YNJ        NaN   \n",
       "55070                               Yingkou     ZYYK       YKH        NaN   \n",
       "55071                              Shenyang     ZYYY       NaN        NaN   \n",
       "55072                               Sealand      NaN       NaN        NaN   \n",
       "55073                      Grande Glorieuse      NaN       NaN        NaN   \n",
       "55074                          Mishima-Mura     RJX7       NaN        NaN   \n",
       "\n",
       "                                   coordinates  \n",
       "0           -74.93360137939453, 40.07080078125  \n",
       "1                       -101.473911, 38.704022  \n",
       "2                  -151.695999146, 59.94919968  \n",
       "3        -86.77030181884766, 34.86479949951172  \n",
       "4                          -91.254898, 35.6087  \n",
       "5                      -97.8180194, 34.9428028  \n",
       "6      -112.16500091552734, 34.305599212646484  \n",
       "7           -116.888000488, 35.350498199499995  \n",
       "8                       -121.763427, 39.427188  \n",
       "9                     -116.4597417, 32.7273736  \n",
       "10                      -104.344002, 40.622202  \n",
       "11       -82.21900177001953, 28.64550018310547  \n",
       "12      -82.34539794921875, 28.846599578857422  \n",
       "13      -80.96920013427734, 27.230899810791016  \n",
       "14       -84.06829833984375, 33.76750183105469  \n",
       "15       -84.73390197753906, 33.88420104980469  \n",
       "16                      -155.980233, 19.832715  \n",
       "17     -116.21399688720703, 48.145301818847656  \n",
       "18                      -101.395994, 39.724028  \n",
       "19        -87.122802734375, 41.644500732421875  \n",
       "20        -89.5604019165039, 41.97840118408203  \n",
       "21        -87.2605972290039, 41.51139831542969  \n",
       "22        -89.1229019165039, 40.02560043334961  \n",
       "23       -94.93049621582031, 38.72779846191406  \n",
       "24      -84.61969757080078, 37.409400939941406  \n",
       "25                       -90.980833, 30.191944  \n",
       "26       -89.70559692382812, 39.66529846191406  \n",
       "27       -92.42939758300781, 30.13629913330078  \n",
       "28       -75.75379943847656, 38.75709915161133  \n",
       "29       -86.41670227050781, 43.94940185546875  \n",
       "...                                        ...  \n",
       "55045                    122.853996, 41.105301  \n",
       "55046             125.684997559, 43.9962005615  \n",
       "55047             122.666944444, 39.2666666667  \n",
       "55048                    120.434998, 41.538101  \n",
       "55049                          126.133, 48.445  \n",
       "55050                    121.718122, 42.069014  \n",
       "55051                    134.366447, 48.199494  \n",
       "55052                 126.25, 45.6234016418457  \n",
       "55053             127.308883667, 50.1716209371  \n",
       "55054                    126.396004, 44.002201  \n",
       "55055         130.464996338, 46.84339904789999  \n",
       "55056                        132.660278, 47.11  \n",
       "55057                          131.193, 45.293  \n",
       "55058    121.06199645996094, 41.10139846801758  \n",
       "55059                129.019125, 47.7520555556  \n",
       "55060        97.0363888889, 32.836388888900004  \n",
       "55061             129.569000244, 44.5241012573  \n",
       "55062               122.43, 52.912777777799995  \n",
       "55063   123.91799926757812, 47.239601135253906  \n",
       "55064                    124.550178, 44.938114  \n",
       "55065             124.720222222, 52.2244444444  \n",
       "55066     121.53900146484375, 38.9656982421875  \n",
       "55067             125.703333333, 42.2538888889  \n",
       "55068   123.48300170898438, 41.639801025390625  \n",
       "55069             129.451004028, 42.8828010559  \n",
       "55070                      122.3586, 40.542524  \n",
       "55071   123.49600219726562, 41.784400939941406  \n",
       "55072                        1.4825, 51.894444  \n",
       "55073  47.296388888900005, -11.584277777799999  \n",
       "55074                    130.270556, 30.784722  \n",
       "\n",
       "[55075 rows x 12 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport = pd.read_csv(RAW_AIRPORT_CODES)\n",
    "df_airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "A quick quality check query confirms the `ident` column is unique and non-null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 55075\n",
      "  ident           type                                name  elevation_ft  \\\n",
      "0   00A       heliport                   Total Rf Heliport          11.0   \n",
      "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
      "2  00AK  small_airport                        Lowell Field         450.0   \n",
      "3  00AL  small_airport                        Epps Airpark         820.0   \n",
      "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
      "\n",
      "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
      "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
      "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
      "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
      "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
      "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
      "\n",
      "  local_code                            coordinates  \n",
      "0        00A     -74.93360137939453, 40.07080078125  \n",
      "1       00AA                 -101.473911, 38.704022  \n",
      "2       00AK            -151.695999146, 59.94919968  \n",
      "3       00AL  -86.77030181884766, 34.86479949951172  \n",
      "4        NaN                    -91.254898, 35.6087  \n",
      "         ident            type                       name  elevation_ft  \\\n",
      "55070     ZYYK  medium_airport      Yingkou Lanqi Airport           0.0   \n",
      "55071     ZYYY  medium_airport    Shenyang Dongta Airport           NaN   \n",
      "55072  ZZ-0001        heliport            Sealand Helipad          40.0   \n",
      "55073  ZZ-0002   small_airport  Glorioso Islands Airstrip          11.0   \n",
      "55074     ZZZZ   small_airport    Satsuma IÅjima Airport         338.0   \n",
      "\n",
      "      continent iso_country iso_region      municipality gps_code iata_code  \\\n",
      "55070        AS          CN      CN-21           Yingkou     ZYYK       YKH   \n",
      "55071        AS          CN      CN-21          Shenyang     ZYYY       NaN   \n",
      "55072        EU          GB     GB-ENG           Sealand      NaN       NaN   \n",
      "55073        AF          TF     TF-U-A  Grande Glorieuse      NaN       NaN   \n",
      "55074        AS          JP      JP-46      Mishima-Mura     RJX7       NaN   \n",
      "\n",
      "      local_code                              coordinates  \n",
      "55070        NaN                      122.3586, 40.542524  \n",
      "55071        NaN   123.49600219726562, 41.784400939941406  \n",
      "55072        NaN                        1.4825, 51.894444  \n",
      "55073        NaN  47.296388888900005, -11.584277777799999  \n",
      "55074        NaN                    130.270556, 30.784722  \n",
      "All keys are unique and non-null.\n"
     ]
    }
   ],
   "source": [
    "quality_check_lookup_csv_files(RAW_AIRPORT_CODES, \"ident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The column `iata` looks like it contains lots of null. It might potentially be join-able to the i94 dataset column `i94port` somehow? (Due to time limitation we are not going to assess that for now. We may come back to future).\n",
    "\n",
    "Conclusion: we will not be using Source 3 in our Data Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Assess Source 4: i94 Lookup CSV Files (Derived from source 2)\n",
    "\n",
    "(We derived this new source as part of the Source 2 assessment. We already conducted a data quality check against this new source at the end of source 2 assessment and have confirmed the key values are non-null and unique. In our data model, we will left join the 5 lookup tables from this source, to primary source 1 table. So the analyst downstream may get a richer experience when analysing the i94 dataset).\n",
    "\n",
    "* File format: `.CSV` File (Delimiter: comma `,`)\n",
    "* Content: contains tabular lookup mapping between the (numeric or character) codes and categorical string values. This covers the i94 Immigration Dataset columns: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`. These are dimension tables.\n",
    "* Remark: we will derive these CSV lookup tables with a Python script, using source (2 - SAS program file) above as input.\n",
    "* Official source location: we generate these lookup tables from source (2) the SAS Program Text file as part of this project. We \n",
    "* Relative path: `raw_input_data/i94_lookup_csv/lookup_<field>.csv`, where `<field>` may be:\n",
    "    * `i94addr`\n",
    "    * `i94cntyn` - may be used to lookup: `i94cit`, `i94res`\n",
    "    * `i94mode`\n",
    "    * `i94port`\n",
    "    * `i94visa`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "(Map out the conceptual data model and explain we chose that model.)\n",
    "\n",
    "The eventual output that will be exposed the the analyst will be a consolidated (denormalised), and partitioned datalake. We will call this the i94 Enhanced (i94e) Datalake.\n",
    "\n",
    "This dataset will be partitioned by arrival year and month (i.e. the `i94yr` and and `i94mon` columns). Analysts will be able to analyse subset of the data easily, by reading in just the required time-range (specified by these two paritioning columns.\n",
    "\n",
    "The i94e dataset will contain all the columns that are in the original i94 dataset, with the following modifications:\n",
    "\n",
    "* a new column called `GUID`. This would be a concatenation of `i94yr`, `i94mon`, and `cicid`, with a \"-\" delimiter. This is to ensure we continue to have a globally unique identifier, in the case the analysts decides to analyse across multiple `i94yr` and `i94mon`.\n",
    "* In additional to (the currently in SAS format) `arrdate` and `depdate`, we will create the equivalent `arrdate_pydt` and `depdate_pydt` (in Python datetime format). We decide to preserve the original SAS format date (rather than replacing them) to give analysts the additional opportunities to further validate our SAS-to-Python datetime conversion. There could be patterns that we have overlooked and we do not wish to hide away these potentially patterns or features, which may help us improve.\n",
    "* In additional to the current categorial codes, below, we will have additional new columns showing the associated string value. This is to save analysts from having to do potentially expensive data joins. (the CSV lookup tables are there should analysts will to do more QA on this). We keep the original columns for transparency and additional QA opportunities.\n",
    "    * `i94addr` -> add additional column `i94addr_value`\n",
    "    * `i94cit`  -> add additional column `i94cit_value`\n",
    "    * `i94res`  -> add additional column `i94res_value`\n",
    "    * `i94mode` -> add additional column `i94mode_value`\n",
    "    * `i94port` -> add additional column `i94port_value`\n",
    "    * `i94visa` -> add additional column `i94visa_value`\n",
    "* `count` is always 1. It is safe to drop this column.\n",
    "* whenever possible we will rename fields to something more meaningful\n",
    "* whenever possible, cast double numeric values as integer or long (big integer) if the field is integer like.\n",
    "\n",
    "We will refrain from manually modify the cell values where possible there are possible data quality issues. We keep the principal of maximum transparency at this early phase of the analytical process. Any abnormality are preserved in the dataset. We will flag these however via a seperate abnormality detection process. Though more work yet to be done on that abnormality detection process (out of scope of this project), we have provided an indication of some potential approaches. For instance, we have exported 1000 rows of group-by aggregation stats via the CSV files: `/raw_input_data/i94_sample_groupby_stats_csv/df_<field>_agg_1.csv`. Analysts may use these as a starting point and investigation further before making decisions (such as whether to remove the records, or manually clean the records). It is also worth checking if the total number of abnmormality is significant - if the numbers are small enough, the solution could be as simply as moving those abnormal records to a quarantine area and inspect / treat them seperately. \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "(List the steps necessary to pipeline the data into the chosen data model)\n",
    "\n",
    "**NOTE: In this Project we focus on Local Mode. S3 mode is out of scope of this project but we set the foundation for us to easily revisit in future.**\n",
    "\n",
    "We use the `aws_dev.cfg` file to control directory and file paths (and other static global contstant). Note that in this file we have the following 3 domains split over two modes (Local and S3 modes):\n",
    "\n",
    "Local mode :\n",
    "\n",
    "* `[DATA_PATHS_UDACITY]`: this is where the large datasets are currently stored on the Udacity repository. (we could move these somewhere else if we need to in future).\n",
    "* `[DATA_PATHS_LOCAL]`: local mode - this entire project development is being run on the local Udacity Virtual Spark Cluster Environment. There are no AWS elements here. We want to use this environment to build out our analytics and ETL prototypes without having to worry too much about AWS costs.\n",
    "\n",
    "S3 Mode:\n",
    "\n",
    "* `[DATA_PATHS_S3]`: S3 mode - this domain contains all the global variables as defined for `[DATA_PATHS_UDACITY]` and `DATA_PATHS_LOCAL` and when local mode has been proven a success, we can migrate our local mode code repository to S3 mode. We want to design our codebase in a way such that the migration from Local to S3 mode will not require any major code changes. The desirable architecture is to have a simple pointer to anable us to switch between Local and S3 mode. A novel architecture may be to have a code block at the top to enable us to do this \"switching\" between Local Mode (to run on Udacity virtual server and retrieve files from accessible file systems) and remote AWS mode (to run on EMR and retrieve files form S3)\n",
    "\n",
    "```python\n",
    "LOCAL_MODE = True\n",
    "\n",
    "if LOCAL_MODE:\n",
    "    # use the domains: [DATA_PATHS_UDACITY] and [DATA_PATHS_LOCAL]\n",
    "    PAR_I94_DIR_BY_I94YR_I94MON = config_dev.get('DATA_PATHS_UDACITY', 'RAW_I94_MTHLY_SAS7BDAT_DIR')\n",
    "    PAR_I94_DIR_BY_I94YR_I94MON = config_dev.get('DATA_PATHS_LOCAL', 'PAR_I94_DIR_BY_I94YR_I94MON')\n",
    "    # and so on\n",
    "    \n",
    "else:\n",
    "    # use the domain: [DATA_PATHS_S3]\n",
    "    PAR_I94_DIR_BY_I94YR_I94MON = config_dev.get('DATA_PATHS_S3', 'RAW_I94_MTHLY_SAS7BDAT_DIR')\n",
    "    PAR_I94_DIR_BY_I94YR_I94MON = config_dev.get('DATA_PATHS_S3', 'PAR_I94_DIR_BY_I94YR_I94MON')    \n",
    "    # and so on    \n",
    "```\n",
    "\n",
    "The general pipeline looks like this (we effectively turn our Notebook codes above into re-runnable python modules and scripts)\n",
    "\n",
    "1. Store the on-going monthly partitioned primary i94 SAS datsets (we currently have 12 of these for 2016) in `RAW_I94_MTHLY_SAS7BDAT_DIR` (our large sas7bdat data store).\n",
    "2. Store the static SAS code (a text-file) in: `RAW_I94_SAS_CODE_FILE`\n",
    "3. Run a PySpark job to convert the 12x i94 SAS7BDAT files from `RAW_I94_MTHLY_SAS7BDAT_DIR`, and directly parse into parquet files in `PAR_I94_DIR_BY_I94YR_I94MON`. No transformation is performed at this stage.\n",
    "    * Job name: `i94_sas7bdat_to_parquet.py`\n",
    "4. Run a (non PySpark) Python job to extract i94 lookup logic from the the static SAS code, and directly store vlookup output to `RAW_I94_LOOKUP_CSV_DIR`.\n",
    "    * Job name: `i94_sas_code_to_vlookup_csv.py`\n",
    "5. (optional) Run a EDA python job to read in the i94 parquet files (created in 3) and run group-by aggregation analyisis. Generate the group-by aggreagation CSV files at: `RAW_I94_DIR_SAMPLE_GROUPBY_STATS_CSV`:\n",
    "    * Job name: `i94_EDA_agg.py`\n",
    "6. Run a PySpark job to build our Enhanced i94 Datalake. This job effectively grab all columns from the i94 parquet (created in step 3 above), left join the vlookup tables (add the vlookup fields), do some transformation (add new GUID, arrdate_pydt, and dep_pydate). If we want we can create more features here also: e.g. arrival_week, arrival_day_of_week.\n",
    "    * Job name: `i94_040_build_i94e_datalake.py`\n",
    "    \n",
    "(after that we will have data quality checks, and sample analytical queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "\n",
    "### 4.1 Create the data model\n",
    "\n",
    "(Build the data pipelines to create the data model.)\n",
    "\n",
    "Since we already have covered step 1 to 5 of the pipeline steps, we only have step 6 to develop. Once we have developed step 6 we should be in a good place to start moving the notebook clodes into standalone Python modudles and scripts.\n",
    "\n",
    "This is the last part of the pipeline process that is missing:\n",
    "\n",
    "6. Run a PySpark job to build our Enhanced i94 Datalake. This job effectively grab all columns from the i94 parquet (created in step 3 above), left join the vlookup tables (add the vlookup fields), do some transformation (add new GUID, arrdate_pydt, and dep_pydate). If we want we can create more features here also: e.g. arrival_week, arrival_day_of_week.\n",
    "\n",
    "Note: after some experiments, we figure that it is more performant to process one (monthly) partition at a time (e.g. around 3.3m rows at a time) for 12 times (for the 12 months), rather than the entire dataset (40 million rows over 12 months) once. Two reasons for this:\n",
    "\n",
    "1. say 3 partitions succeeded and 4th partition failed, we can resume from 4th partiion rather than from the beginning.\n",
    "2. probably less resource consumption. We are processing 3.3 million rows at a time, rather than 40 million rows at a time.\n",
    "3. allow more incremental logging. So we can see progress better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR_I94E_DIR: par_output_data/i94e/\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94ADDR: raw_input_data/i94_lookup_csv/lookup_i94addr.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94CNTYL: raw_input_data/i94_lookup_csv/lookup_i94cntyl.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94PORT: raw_input_data/i94_lookup_csv/lookup_i94port.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94MODE: raw_input_data/i94_lookup_csv/lookup_i94mode.csv\n",
      "RAW_I94_LOOKUP_CSV_FILE_I94VISA: raw_input_data/i94_lookup_csv/lookup_i94visa.csv\n"
     ]
    }
   ],
   "source": [
    "# Build and write i94e in chunks.\n",
    "\n",
    "PAR_I94E_DIR = config_dev.get('DATA_PATHS_LOCAL', 'PAR_I94E_DIR')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94ADDR=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94ADDR')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94CNTYL=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94CNTYL')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94PORT=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94PORT')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94MODE=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94MODE')\n",
    "RAW_I94_LOOKUP_CSV_FILE_I94VISA=config_dev.get('DATA_PATHS_LOCAL', 'RAW_I94_LOOKUP_CSV_FILE_I94VISA')\n",
    "print(f\"PAR_I94E_DIR: {PAR_I94E_DIR}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94ADDR: {RAW_I94_LOOKUP_CSV_FILE_I94ADDR}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94CNTYL: {RAW_I94_LOOKUP_CSV_FILE_I94CNTYL}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94PORT: {RAW_I94_LOOKUP_CSV_FILE_I94PORT}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94MODE: {RAW_I94_LOOKUP_CSV_FILE_I94MODE}\")\n",
    "print(f\"RAW_I94_LOOKUP_CSV_FILE_I94VISA: {RAW_I94_LOOKUP_CSV_FILE_I94VISA}\")\n",
    "\n",
    "def i94e_build_write_in_chunks(\n",
    "    spark,\n",
    "    i94yr_list: list,\n",
    "    i94mon_list: list,\n",
    "    partition_by_list: list = None\n",
    "    ):\n",
    "    \"\"\" Create i94e partiion by partition, and append to base \"\"\"\n",
    "\n",
    "    # Read the primary i94 parquet\n",
    "    df_lkup_addr = spark.read.csv(RAW_I94_LOOKUP_CSV_FILE_I94ADDR, header=True)\n",
    "    df_lkup_cntyl = spark.read.csv(RAW_I94_LOOKUP_CSV_FILE_I94CNTYL, header=True)\n",
    "    df_lkup_port = spark.read.csv(RAW_I94_LOOKUP_CSV_FILE_I94PORT, header=True)\n",
    "    df_lkup_mode = spark.read.csv(RAW_I94_LOOKUP_CSV_FILE_I94MODE, header=True)\n",
    "    df_lkup_visa = spark.read.csv(RAW_I94_LOOKUP_CSV_FILE_I94VISA, header=True)\n",
    "    \n",
    "    df_lkup_addr.createOrReplaceTempView('addr')\n",
    "    df_lkup_cntyl.createOrReplaceTempView('cntyl')\n",
    "    df_lkup_mode.createOrReplaceTempView('mode')\n",
    "    df_lkup_port.createOrReplaceTempView('port')\n",
    "    df_lkup_visa.createOrReplaceTempView('visa')\n",
    "    \n",
    "    print(f\"Build and write i94e in chunks (by: {partition_by_list})\")\n",
    "    for y in i94yr_list:\n",
    "        \n",
    "        for m in i94mon_list:\n",
    "            print(f\"Partition: i94yr={y} and i94mm={m}\")\n",
    "            # Read the primary i94 parquet (for a partition)\n",
    "            df_i94 = spark.read.parquet(PAR_I94_DIR_BY_I94YR_I94MON).where(f\"i94yr={y} and i94mon={m}\")\n",
    "\n",
    "            # Create the python datetime here (we might be able to do it within spark SQL however)\n",
    "            udf_datetime_from_sas = F.udf(lambda x: convert_datetime(x), T.DateType())\n",
    "            df_i94 = df_i94.withColumn('arrdate_pydt', udf_datetime_from_sas(df_i94.arrdate))\n",
    "            df_i94 = df_i94.withColumn('depdate_pydt', udf_datetime_from_sas(df_i94.depdate))\n",
    "\n",
    "            # From i94 left join lookup tables (for that particular partition)\n",
    "            df_i94.createOrReplaceTempView('main')\n",
    "            df_i94e = spark.sql(\"\"\"\n",
    "            select\n",
    "                concat(\n",
    "                    cast(cast(main.i94yr as int) as string),\n",
    "                    '-',\n",
    "                    lpad(cast(cast(main.i94mon as int) as string), 2, '0'),\n",
    "                    '-',\n",
    "                    cast(cast(main.cicid as int) as string)\n",
    "                ) as guid,\n",
    "\n",
    "                main.i94yr,\n",
    "                main.i94mon,\n",
    "                main.cicid,\n",
    "\n",
    "                main.arrdate_pydt as arrival_date,\n",
    "                main.depdate_pydt as departure_date,\n",
    "\n",
    "                main.i94addr,\n",
    "                addr.value as i94addr_value,\n",
    "\n",
    "                main.i94cit,\n",
    "                cntyl_cit.value as i94cit_value,\n",
    "\n",
    "                main.i94res,\n",
    "                cntyl_res.value as i94res_value,\n",
    "\n",
    "                main.i94mode,\n",
    "                mode.value as i94mode_value,\n",
    "\n",
    "                main.i94port,\n",
    "                port.value as i94port_value,    \n",
    "\n",
    "                main.i94visa,\n",
    "                visa.value as i94visa_value,\n",
    "                main.visatype,\n",
    "                main.visapost,\n",
    "               \n",
    "                main.gender,\n",
    "                main.biryear,\n",
    "                main.i94bir,\n",
    "                main.occup,\n",
    "\n",
    "                main.admnum,\n",
    "                main.insnum,    \n",
    "                main.entdepa,\n",
    "                main.entdepd,\n",
    "                main.entdepu,\n",
    "                main.matflag,\n",
    "                main.dtadfile,\n",
    "                main.dtaddto,\n",
    "                main.airline,\n",
    "                main.fltno,\n",
    "                main.arrdate,\n",
    "                main.depdate\n",
    "\n",
    "            from main\n",
    "            left join addr on main.i94addr = addr.key\n",
    "            left join cntyl as cntyl_cit on main.i94cit = cntyl_cit.key\n",
    "            left join cntyl as cntyl_res on main.i94res = cntyl_res.key\n",
    "            left join mode on main.i94mode = mode.key\n",
    "            left join port on main.i94port = port.key\n",
    "            left join visa on main.i94visa = visa.key\n",
    "            \"\"\")\n",
    "\n",
    "            # append to base\n",
    "            df_i94e.write.mode(\"append\").partitionBy(*partition_by_list).parquet(PAR_I94E_DIR)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!rm -r {PAR_I94E_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and write i94e in chunks (by: ['i94yr', 'i94mon'])\n",
      "Partition: i94yr=2016 and i94mm=1\n",
      "Partition: i94yr=2016 and i94mm=2\n",
      "Partition: i94yr=2016 and i94mm=3\n",
      "Partition: i94yr=2016 and i94mm=4\n",
      "Partition: i94yr=2016 and i94mm=5\n",
      "Partition: i94yr=2016 and i94mm=6\n",
      "Partition: i94yr=2016 and i94mm=7\n",
      "Partition: i94yr=2016 and i94mm=8\n",
      "Partition: i94yr=2016 and i94mm=9\n",
      "Partition: i94yr=2016 and i94mm=10\n",
      "Partition: i94yr=2016 and i94mm=11\n",
      "Partition: i94yr=2016 and i94mm=12\n"
     ]
    }
   ],
   "source": [
    "i94e_build_write_in_chunks(\n",
    "    spark,\n",
    "    i94yr_list=[2016],\n",
    "    i94mon_list=[1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "    partition_by_list=[\"i94yr\", \"i94mon\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks\n",
    "\n",
    "Though we have already run some sanity checks previously, for completness we will include some high level data quality checks here. We can add more as we learn.\n",
    "\n",
    "1. `df_i94e` (after left join) must have the same rows as `df_i94` (before left join).\n",
    "2. `cicid` must be unique for each partition (`i94yr`, `i94mon`). i.e total row counts before and after a distinct operation of this column, must be the same.\n",
    "\n",
    "(We can add more automated data quality checks in future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_i94e_datalake_1(\n",
    "        spark,\n",
    "        i94yr_list: list,\n",
    "        i94mon_list: list,\n",
    "    ):\n",
    "    \"\"\" Data Quality Check \"\"\"\n",
    "    print(f\"Test 1: f_i94e (after left join) must have the same rows as df_i94 (before left join).\")\n",
    "    for y in i94yr_list:\n",
    "        for m in i94mon_list:\n",
    "            print(f\"Partition: i94yr={y} and i94mm={m}\")\n",
    "            # Read the primary i94 parquet (for a partition)\n",
    "            df_i94 = spark.read.parquet(PAR_I94_DIR_BY_I94YR_I94MON).where(f\"i94yr={y} and i94mon={m}\")\n",
    "            df_i94e = spark.read.parquet(PAR_I94E_DIR).where(f\"i94yr={y} and i94mon={m}\")\n",
    "            assert df_i94e.count() == df_i94.count()\n",
    "            print(\"Partition Pass!\")\n",
    "            \n",
    "    print(\"Overall: Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: f_i94e (after left join) must have the same rows as df_i94 (before left join).\n",
      "Partition: i94yr=2016 and i94mm=1\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=2\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=3\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=4\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=5\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=6\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=7\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=8\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=9\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=10\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=11\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=12\n",
      "Partition Pass!\n",
      "Overall: Passed!\n"
     ]
    }
   ],
   "source": [
    "test_i94e_datalake_1(spark, [2016], [1,2,3,4,5,6,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_i94e_datalake_2(\n",
    "        spark,\n",
    "        i94yr_list: list,\n",
    "        i94mon_list: list,\n",
    "    ):\n",
    "    \"\"\" Data Quality Check \"\"\"\n",
    "    print(f\"\"\"\n",
    "        Test 2: cicid must be unique for each partition (`i94yr`, `i94mon`).\\ \n",
    "        i.e total row counts before and after a distinct operation of this column, must be the same.\"\"\")\n",
    "    for y in i94yr_list:\n",
    "        for m in i94mon_list:\n",
    "            print(f\"Partition: i94yr={y} and i94mm={m}\")\n",
    "            # Read the primary i94 parquet (for a partition)\n",
    "            df_i94 = spark.read.parquet(PAR_I94_DIR_BY_I94YR_I94MON).where(f\"i94yr={y} and i94mon={m}\")\n",
    "            df_i94e = spark.read.parquet(PAR_I94E_DIR).where(f\"i94yr={y} and i94mon={m}\")\n",
    "            assert df_i94e.select(\"cicid\").count() == df_i94e.select(\"cicid\").distinct().count()\n",
    "            print(\"Partition Pass!\")\n",
    "            \n",
    "    print(\"Overall: Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Test 2: cicid must be unique for each partition (`i94yr`, `i94mon`).\\ \n",
      "        i.e total row counts before and after a distinct operation of this column, must be the same.\n",
      "Partition: i94yr=2016 and i94mm=1\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=2\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=3\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=4\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=5\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=6\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=7\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=8\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=9\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=10\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=11\n",
      "Partition Pass!\n",
      "Partition: i94yr=2016 and i94mm=12\n",
      "Partition Pass!\n",
      "Overall: Passed!\n"
     ]
    }
   ],
   "source": [
    "test_i94e_datalake_2(spark, [2016], [1,2,3,4,5,6,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Sample query an analyst might run\n",
    "\n",
    "Sample query: during June through August 2016, give me a breakdown of `i94visa_value` (a field we obtained earlier by using a lookup table programmatically created from a SAS code file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR_I94E_DIR: par_output_data/i94e/\n",
      "root\n",
      " |-- guid: string (nullable = true)\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94addr_value: string (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94cit_value: string (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94res_value: string (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94mode_value: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94port_value: string (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- i94visa_value: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PAR_I94E_DIR = config_dev.get('DATA_PATHS_LOCAL', 'PAR_I94E_DIR')\n",
    "print(f\"PAR_I94E_DIR: {PAR_I94E_DIR}\")\n",
    "\n",
    "df_i94e = spark.read.parquet(PAR_I94E_DIR).where(f\"i94yr=2016 and i94mon in (6,7,8)\")\n",
    "df_i94e.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+-------+\n",
      "| i94yr|i94mon|i94visa_value|records|\n",
      "+------+------+-------------+-------+\n",
      "|2016.0|   6.0|     Business| 487128|\n",
      "|2016.0|   6.0|     Pleasure|3010141|\n",
      "|2016.0|   6.0|      Student|  77720|\n",
      "|2016.0|   7.0|     Business| 415876|\n",
      "|2016.0|   7.0|     Pleasure|3731242|\n",
      "|2016.0|   7.0|      Student| 117913|\n",
      "|2016.0|   8.0|     Business| 376261|\n",
      "|2016.0|   8.0|     Pleasure|3286196|\n",
      "|2016.0|   8.0|      Student| 441113|\n",
      "+------+------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94e.createOrReplaceTempView('df_i94e')\n",
    "spark.sql(\"\"\"\n",
    "select\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    i94visa_value,\n",
    "    count(*) as records\n",
    "from df_i94e\n",
    "group by 1,2,3\n",
    "order by 1,2,3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are more different types of SQL Queires we can try run, but you get the idea! (we retrict our time range via `i94yr` and `i94mon` to save computational cost, and we are able to use a new field `i94visa_value` that was not in the original i94 table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+-------+\n",
      "| i94yr|i94mon|visatype|records|\n",
      "+------+------+--------+-------+\n",
      "|2016.0|   6.0|      B1| 196330|\n",
      "|2016.0|   6.0|      B2|1463607|\n",
      "|2016.0|   6.0|      CP|  15401|\n",
      "|2016.0|   6.0|     CPL|     19|\n",
      "|2016.0|   6.0|      E1|   3826|\n",
      "|2016.0|   6.0|      E2|  19474|\n",
      "|2016.0|   6.0|      F1|  72278|\n",
      "|2016.0|   6.0|      F2|   4329|\n",
      "|2016.0|   6.0|     GMB|    346|\n",
      "|2016.0|   6.0|     GMT| 101149|\n",
      "|2016.0|   6.0|       I|   3180|\n",
      "|2016.0|   6.0|      I1|    250|\n",
      "|2016.0|   6.0|      M1|   1090|\n",
      "|2016.0|   6.0|      M2|     23|\n",
      "|2016.0|   6.0|     SBP|      6|\n",
      "|2016.0|   6.0|      WB| 263722|\n",
      "|2016.0|   6.0|      WT|1429959|\n",
      "|2016.0|   7.0|      B1| 175635|\n",
      "|2016.0|   7.0|      B2|1767133|\n",
      "|2016.0|   7.0|      CP|  18865|\n",
      "+------+------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another Example query\n",
    "spark.sql(\"\"\"\n",
    "select\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    visatype,\n",
    "    count(*) as records\n",
    "from df_i94e\n",
    "group by 1,2,3\n",
    "order by 1,2,3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------+--------+-------+\n",
      "| i94yr|i94mon|i94visa_value|visatype|records|\n",
      "+------+------+-------------+--------+-------+\n",
      "|2016.0|   6.0|     Business|      B1| 196330|\n",
      "|2016.0|   6.0|     Business|      E1|   3826|\n",
      "|2016.0|   6.0|     Business|      E2|  19474|\n",
      "|2016.0|   6.0|     Business|     GMB|    346|\n",
      "|2016.0|   6.0|     Business|       I|   3180|\n",
      "|2016.0|   6.0|     Business|      I1|    250|\n",
      "|2016.0|   6.0|     Business|      WB| 263722|\n",
      "|2016.0|   6.0|     Pleasure|      B2|1463607|\n",
      "|2016.0|   6.0|     Pleasure|      CP|  15401|\n",
      "|2016.0|   6.0|     Pleasure|     CPL|     19|\n",
      "|2016.0|   6.0|     Pleasure|     GMT| 101149|\n",
      "|2016.0|   6.0|     Pleasure|     SBP|      6|\n",
      "|2016.0|   6.0|     Pleasure|      WT|1429959|\n",
      "|2016.0|   6.0|      Student|      F1|  72278|\n",
      "|2016.0|   6.0|      Student|      F2|   4329|\n",
      "|2016.0|   6.0|      Student|      M1|   1090|\n",
      "|2016.0|   6.0|      Student|      M2|     23|\n",
      "|2016.0|   7.0|     Business|      B1| 175635|\n",
      "|2016.0|   7.0|     Business|      E1|   4976|\n",
      "|2016.0|   7.0|     Business|      E2|  27711|\n",
      "|2016.0|   7.0|     Business|     GMB|    106|\n",
      "|2016.0|   7.0|     Business|       I|   2915|\n",
      "|2016.0|   7.0|     Business|      I1|    208|\n",
      "|2016.0|   7.0|     Business|      WB| 204325|\n",
      "|2016.0|   7.0|     Pleasure|      B2|1767133|\n",
      "|2016.0|   7.0|     Pleasure|      CP|  18865|\n",
      "|2016.0|   7.0|     Pleasure|     CPL|     13|\n",
      "|2016.0|   7.0|     Pleasure|     GMT|  95021|\n",
      "|2016.0|   7.0|     Pleasure|     SBP|      2|\n",
      "|2016.0|   7.0|     Pleasure|      WT|1850208|\n",
      "|2016.0|   7.0|      Student|      F1| 108531|\n",
      "|2016.0|   7.0|      Student|      F2|   8016|\n",
      "|2016.0|   7.0|      Student|      M1|   1299|\n",
      "|2016.0|   7.0|      Student|      M2|     67|\n",
      "|2016.0|   8.0|     Business|      B1| 166079|\n",
      "|2016.0|   8.0|     Business|      E1|   5108|\n",
      "|2016.0|   8.0|     Business|      E2|  28755|\n",
      "|2016.0|   8.0|     Business|     GMB|     98|\n",
      "|2016.0|   8.0|     Business|       I|   3001|\n",
      "|2016.0|   8.0|     Business|      I1|    216|\n",
      "+------+------+-------------+--------+-------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another Example query\n",
    "spark.sql(\"\"\"\n",
    "select\n",
    "    i94yr,\n",
    "    i94mon,\n",
    "    i94visa_value,\n",
    "    visatype,\n",
    "    count(*) as records\n",
    "from df_i94e\n",
    "group by 1,2,3,4\n",
    "order by 1,2,3,4\n",
    "\"\"\").show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "\n",
    "(Disclaimer: the following data dictionary is subject to error and may be updated over time when more insights are acquired)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "Sample code to read the `i94e` datalake into Spark DataFrame:\n",
    "\n",
    "```.python\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "\n",
    "config_dev = configparser.ConfigParser()\n",
    "config_dev.read_file(open('aws_dev.cfg'))\n",
    "PAR_I94E_DIR = config_dev.get('DATA_PATHS_LOCAL', 'PAR_I94E_DIR')\n",
    "\n",
    "df_i94e = spark.read.parquet(PAR_I94E_DIR).where(f\"i94yr=2016 and i94mon in (6,7,8)\")\n",
    "df_i94e.printSchema()\n",
    "```\n",
    "\n",
    "Location of the `i94e` (i94 Enhanced) Datalake:\n",
    "\n",
    "* in local mode: use `config_dev.get('DATA_PATHS_LOCAL', 'PAR_I94E_DIR')`\n",
    "* in aws mode (not yet developed): use `config_dev.get('DATA_PATHS_S3', 'PAR_I94E_DIR')`\n",
    "\n",
    "Fields and descriptions.\n",
    "\n",
    "* `guid`: a global unique identifier (GUID). This is a derived string field: we concatenate `i94yr`, `i94mon`, `cicid` with the `-` seperator. This field is unique across time range. (derived)\n",
    "* `cicid` : CICID is an unique ID for each US arrival-departure record which the visitor has filled in an i94 form. All arrival-departure CICID is unique. CICID is reset to 1 at the 1st of each arrival month. (From Source 1)\n",
    "* `arrival_date`: this is the Python Datetime equivalent of the upstream field `arrdate` (in SAS format). Analyst may find this datetime field more useful for analysis. (derived)\n",
    "* `departure_date`: this is the Python Datetime equivalent of the upstream field `depdate` (in SAS format). Analyst may find this datetime field more useful for analysis. (derived)\n",
    "* `i94addr`: Visitor's residence (or accomodation) in the US, represented in 1-2 character codes. (official schema to be identified). (From Source 1)\n",
    "* `i94addr_value`: Visitor's residence (or accomodation) in the US, Represented as a human readable string name. (From Source 2 and 4)\n",
    "* `i94cit` : Visitor's country of citizenship. (i.e. as per passport). Represented as a numeric code. (From Source 1)\n",
    "* `i94cit_value`: Visitor's country of citizenship. Represented as a human readable string name. (From Source 2 and 4)\n",
    "* `i94res` : Visitor's country of residence / country travelling into US from. Represented as a numeric code. (From Source 1)\n",
    "* `i94res_value` : Visitor's country of residence / country travelling into US from. Represented as a human readable string name. (From Source 2 and 4)\n",
    "* `i94mode`: mode of travelling into the US (sea, land, air, or not reported). Represented as a numeric code. (From Source 1)\n",
    "* `i94mode_value`: mode of travelling into the US (sea, land, air, or not reported). Represented as a human readable string name. (From Source 2 and 4)\n",
    "* `i94port`: port of entry into the US. Represented as a string code. (From Source 1)\n",
    "* `i94port_value`: port of entry into the US. Represented as a human readable string name. (From Source 2 and 4)\n",
    "* `i94visa` : the high-level visa category (1, 2, 3). Represented as a numeric code. (From Source 1)\n",
    "* `i94visa_value` : the high-level visa category (Business, Pleasure, Student). Represented as a human readable string name. (From Source 2 and 4)\n",
    "* `visatype` : A lower level visa type. Possible correlation with the high level category? (From Source 1)\n",
    "* `visapost` : Department of State where where Visa was issued - CIC does not use. (From Source 1)\n",
    "* `gender` : Non-immigrant sex. (From Source 1)\n",
    "* `biryear` : birth year of the immigrant on the i94 form. (From Source 1)\n",
    "* `i94bir` : 4 digit year of immigrant birth. (From Source 1)\n",
    "* `occup` : Occupation that will be performed in U.S. - CIC does not use. (From Source 1)\n",
    "* `admnum` : Every I-94 record has an eleven-digit admission number. This number may be needed at the Department of Motor Vehicles and for employment purposes, but it is not a number that needs to be memorized. In fact, a new I-94 number will be given each time the student or scholar re-enters the United States. (From Source 1)\n",
    "* `insnum` : INS number. (Is this Insurance Number?). (From Source 1)\n",
    "* `entdepa` : Arrival Flag - admitted or paroled into the U.S. - CIC does not use. (From Source 1)\n",
    "* `entdepd` : Departure Flag - Departed, lost I-94 or is deceased - CIC does not use. (From Source 1)\n",
    "* `entdepu` : Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use. (From Source 1)\n",
    "* `matflag` : Match flag - Match of arrival and departure records. (From Source 1)\n",
    "* `dtadfile` : Character Date Field - Date added to I-94 Files - CIC does not use. (From Source 1)\n",
    "* `dtaddto` : Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use. (From Source 1)\n",
    "* `airline` : Airline used to arrive in U.S. (From Source 1)\n",
    "* `fltno` : Flight number of Airline used to arrive in U.S. (From Source 1)\n",
    "* `arrdate`: is date of arrrival (into the US). Currently in SAS Datetime format. (From Source 1)\n",
    "* `depdate`: is date of departure (out of the US). Currently in SAS Datetime format. (From Source 1)\n",
    "* `i94yr` : 4 digit year (of the arrival date into the US). This value should match the year of `arrival_date` perfectly. (From Source 1)\n",
    "* `i94mon` : Numeric month (of the arrival date into the US). This value should match the month of `arrival_date` perfectly. (From Source 1)\n",
    "\n",
    "Sources (original raw data):\n",
    "\n",
    "1. Source 1: i94 Immigration Data (2016) - SAS Datasets\n",
    "    * File format: `.SAS7BDAT` SAS Binary Data File.\n",
    "    * Content: contains over 40 million arrival (into USA) records from the i94 forms during 2016.\n",
    "    * Remark: this is our primary table. (contains fact and dimension data)\n",
    "    * Official source location: [US National Tourism and Trade Office.](https://travel.trade.gov/research/reports/i94/historical/2016.html). Note: it appears that the URL is now re-directed to another location. For this reason, we will stick with the Udacity Path below (that contains the downloaded dataset).\n",
    "    * Udacity Directory: `/data/18-83510-I94-Data-2016`. It contains 12 SAS Dataset (1 for each arrival month during 2016). Each SAS Dataset has a naming convention like this: `i94_<mmm>16_sub.sas7bdat`, where `mmm` is in this list: `['apr','aug','dec','feb','jan','jul','jun','mar','may','nov','oct','sep']`.\n",
    "    \n",
    "2. Source 2: A SAS Program file that contains the lookup of codes to categorical string values\n",
    "    * File format: `.SAS` Text File (containing SAS Code Syntax)\n",
    "    * Content: this is a semi-structured text file that contains lookup logic from (numeric or character) codes to categorical string values, for the i94 Immigration Dataset columns: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`.\n",
    "    * Remark: we will have an automated Python script to read in this text file, and auto derive CSV lookup tables (as descibed in bullet point 4 below).    \n",
    "    * Official source location (no documentations identified. Likely come from the same source as source (1) above.)\n",
    "    * Relative Path: `raw_input_data/i94_proc_format_code_sas/I94_SAS_Labels_Descriptions.SAS`\n",
    "    \n",
    "Derived Raw Data (to be derived from original raw data with Python Script downstream):\n",
    "\n",
    "4. i94 Lookup CSV Files\n",
    "    * File format: `.CSV` File (Delimiter: comma `,`)\n",
    "    * Content: contains tabular lookup mapping between the (numeric or character) codes and categorical string values. This covers the i94 Immigration Dataset columns: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`. These are dimension tables.\n",
    "    * Remark: we will derive these CSV lookup tables with a Python script, using source (2 - SAS program file) above as input.\n",
    "    * Official source location: we generate these lookup tables from source (2) the SAS Program Text file as part of this project. We \n",
    "    * Relative path: `raw_input_data/i94_lookup_csv/lookup_<field>.csv`, where `<field>` may be:\n",
    "        * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Project write-ups and after-thoughts\n",
    "\n",
    "To conclude, we have:\n",
    "\n",
    "* explored and assessed the i94 dataset, which comes in 12 monthly SAS7BDAT files during 2016.\n",
    "* parsed the SAS7BDAT files into partitioned datalake (parquet files) for ease of downstream Spark EDA, analayst, and further ETL.\n",
    "* identified suitable partition fields as arrival year (`i94yr`) and arrival month (`i94mon`).\n",
    "* identified that the global unique identifier is a combination of arrival year (`i94yr`) and arrival month (`i94mon`), and CICID (`cicid`).\n",
    "* identified that `cicid` is unique, for a particular set of arrival year (`i94yr`) and arrival month (`i94mon`).\n",
    "* programmatically generated lookup tables that will enable us to obtain the more human readable string values for these cryptic coded fields: `i94addr`, `i94cit`, `i94res`, `i94mode`, `i94port`, `i94visa`\n",
    "* left joined the lookup tables into our primary i94 dataset and created our i94 enhanced dataset (i94e).\n",
    "* conducted tests along the way: sanity checks, unit tests, and data quality tests.\n",
    "* written up a data dictionary for both original `i94` dataset (to help EDA), as well as our enhanced `i94e` dataset (which will be used by downstream analyst)\n",
    "* provided some example scripts to illustrate how analysts may read in the i94e datalake (parquet file directory) and reduce computational costs via sub-setting using the partitioning fields.\n",
    "* provided a framework template for more downstream analysis and automation.\n",
    "* the entire project has been done on a local spark cluster. The project has been developed for potential AWS opportunities (migration to S3, EMR, etc.) for further scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Rationale for the choice of tools and technologies for the project\n",
    "\n",
    "The incoming i94 SAS7BDAT datasets were about 500 MB each. 12 of them would be roughly 6000 MB (about 6 GB). There are over 40 million rows of data in the dataest which is considered medium-large dataset. A single-threaded library like Pandas usually would be ok to deal with observations of up to around 1-2 million rows (from experience) on a single machine. Anything beyond that Pandas would struggle. (We may do parallel processing with Pandas but not distributed computing. i.e. Pandas only allow us to parallelise processes using the CPU and Memory resource of the same device (a laptop, PC). To make Pandas overcome performance issue there is only one way, vertical scaling (to increase CPU and memory sizes). This would work up to certain point when cost would become exponential at some ceiling with diminishing return.\n",
    "\n",
    "The Spark distributed processing framework would enable us to dice up the large dataset into smaller chunks, and distribute over multiple server nodes for distributed computation. On the storage side, Spark enable us to write the SAS7BDAT dataset into distributed parquet formats. This enables us to break through the single node computation resorce barrier, and scale horrizontally (add more connected nodes with modest-cheap CPU and Memory), and perform computation in a distributed manner.\n",
    "\n",
    "Since we are programming in Python, we use PySpark as an API to access the Spark Framework (intalled on a currently on-prem Udacity Hadoop cluster). From the experments conducted so far, jobs appear to run well without resource issues so far.\n",
    "\n",
    "For a continually growing datalake, the Udacity Hadoop Cluster may eventually hit limitation, and so it might be wise to also look into alternative solutions that will allow us to continue to horizontally scale in a sustainable manner. AWS S3 might be potentially candidate for storage, and AWS EMR might be potentially candiate for (Spark) compute.\n",
    "\n",
    "Whether we use on-prem or AWS infrastructure, our Notebook prove-of-concept would need to be migrated to a lighter weigh and more maintainable Python modules and/or scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### How often the data should be updated and why.\n",
    "\n",
    "The i94 dataset appear to be best updated on a monthly basis (given the partitioning and the unique nature of CICID per month). The lookup CSV appear static (for now) - though we would need to investigate if there is a more dynamic way to obtain this. For example, what if the lookup mapping changes? Do we need to manually maintain the mapping, or could we obtain a regular mapping update somewhere else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Production sceanrio 1: The data was increased by 100x\n",
    "\n",
    "Say if the data is increased by 100x, we are faced with two possible options:\n",
    "\n",
    "* on-prem solution: we can build out a horizontal scaling solution. This might be the preference if data is sensitive and cloud solution is not an option. On-prem solution could also be potentially be cheaper than cloud solution, if designed and planned right. (though on-prem could also be more in-elastic compare to cloud).\n",
    "* cloud solution: migrate the on-prem ETL pipeline to AWS cloud. Use S3 as storage (CSV files, parquet data lake, JSON, etc). And use AWS EMR for ocmpute (Spark ready). EMR is expensive however, and the alternative could be to setup EC2 compute cluster and manually install and maintain Spark there. (Note that there are also other cloud providers out there such as GCP and Azure - we do not have to use AWS).\n",
    "\n",
    "Whichever solution we select, our code needs to be flexible enough to minimise overall overhaul. We discussed previously it would be a good idea to have a switch mechanism that would point to the cloud or on-prem paths via the configuration file. So far our code has only been tested for the on-prem solution, not quite so much for the cloud part. Thoug we have setup `aws_dev.cfg` to paint a picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Production sceanrio 2: The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "If we need on-going ETL updates and dashboarding (rather then one-off like this project), we would need to re-architecture some codes and file-structure so that:\n",
    "\n",
    "1. on-going datalake appending would work as expected. (avoid unintented re-appending / duplications as a resut of running ETL multile times)\n",
    "2. have the ability to re-run failed job and backfilling.\n",
    "3. automated logging, data quality checks.\n",
    "\n",
    "In terms of dashboarding, the ETL process would need to collect stats that will be useful to the dashboard user (e.g. total row counts, column counts, sample records that have passed and/or failed the data quality checks, job run time, job status - success, running, failing, re-running, etc.). It would be useful to store the stats over a period of time to enable engineers to analyse ETL pipeline performances, and explore potential improvement opportunities. We would also need a scheduling, tool such as Apache Airflow, AutoSys, or other scheduling tools. Airflow comes with a default dashboard. We can also try Grafana for more complex dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Production sceanrio 3: The database needed to be accessed by 100+ people.\n",
    "\n",
    "When the same datalake / database / dataset is used by 100+ people (presumably from different functions and businesses also) we need to be aware of the followings:\n",
    "\n",
    "* different people would have different interpretations, expectations, and requirements. From a dataset schema perspective, the design needs to be as generic as possible to accomodate wide range of business needs.\n",
    "* documentations need to be of high quality and regularly updated and maintained.\n",
    "* training sessions and documentatoins are recommended.\n",
    "* from a tech infrastructure perspective, we need to ensure individual users would have smooth experience using the dataset - e.g. shorter response time. We need to ensure infrastructure would be available at a high rate (as defined by SLA).\n",
    "* proper design of database schema: may contain a number of solid ground-truth datasets, with user-specific \"views\" created to enable business needs.\n",
    "* and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# gracefully terminate Spark session (though if we kill the kernal it should do the same thing)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

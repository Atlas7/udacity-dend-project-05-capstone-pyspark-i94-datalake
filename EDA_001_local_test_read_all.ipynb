{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Udacity Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "To setup Spark Environment on Udacity Environment, use the sample code snippet from: https://knowledge.udacity.com/questions/586304\n",
    "\n",
    "Useful links:\n",
    "\n",
    "https://stackoverflow.com/questions/35497069/passing-ipython-variables-as-arguments-to-bash-commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH_MODE = 'DATA_PATHS_LOCAL'\n",
    "assert DATA_PATH_MODE in ['DATA_PATHS_LOCAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Udacity Server) RAW_I94_MTHLY_SAS7BDAT_DIR: /data/18-83510-I94-Data-2016\n",
      "(Udacity Server) RAW_GLOBAL_CITY_LAND_TEMPERATURE: /data2/GlobalLandTemperaturesByCity.csv\n",
      "PROCESSED_I94: sas_data/\n",
      "RAW_I94_SAMPLE: raw_input_data/i94_sample_csv/immigration_data_sample.csv\n",
      "RAW_AIRPORT_CODES: raw_input_data/airport_codes_csv/airport-codes_csv.csv\n",
      "RAW_US_CITY_DEMOGRAPHICS: raw_input_data/us_cities_demographics_csv/us-cities-demographics.csv\n"
     ]
    }
   ],
   "source": [
    "# Dev config from the non-secret configuration file\n",
    "config_dev = configparser.ConfigParser()\n",
    "config_dev.read_file(open('aws_dev.cfg'))\n",
    "\n",
    "RAW_I94_MTHLY_SAS7BDAT_DIR = config_dev.get('DATA_PATHS_UDACITY', 'RAW_I94_MTHLY_SAS7BDAT_DIR')\n",
    "RAW_GLOBAL_CITY_LAND_TEMPERATURE = config_dev.get('DATA_PATHS_UDACITY', 'RAW_GLOBAL_CITY_LAND_TEMPERATURE')\n",
    "\n",
    "PROCESSED_I94_DIR = config_dev.get(DATA_PATH_MODE, 'PROCESSED_I94_DIR')\n",
    "RAW_I94_SAMPLE = config_dev.get(DATA_PATH_MODE, 'RAW_I94_SAMPLE')\n",
    "RAW_AIRPORT_CODES = config_dev.get(DATA_PATH_MODE, 'RAW_AIRPORT_CODES')\n",
    "RAW_US_CITY_DEMOGRAPHICS = config_dev.get(DATA_PATH_MODE, 'RAW_US_CITY_DEMOGRAPHICS')\n",
    "\n",
    "print(f\"(Udacity Server) RAW_I94_MTHLY_SAS7BDAT_DIR: {RAW_I94_MTHLY_SAS7BDAT_DIR}\")\n",
    "print(f\"(Udacity Server) RAW_GLOBAL_CITY_LAND_TEMPERATURE: {RAW_GLOBAL_CITY_LAND_TEMPERATURE}\")\n",
    "print(f\"PROCESSED_I94: {PROCESSED_I94_DIR}\")\n",
    "print(f\"RAW_I94_SAMPLE: {RAW_I94_SAMPLE}\")\n",
    "print(f\"RAW_AIRPORT_CODES: {RAW_AIRPORT_CODES}\")\n",
    "print(f\"RAW_US_CITY_DEMOGRAPHICS: {RAW_US_CITY_DEMOGRAPHICS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Might not be needed? (Tested not required! But keeping here for info in case we do.)\n",
    "# Got below from https://knowledge.udacity.com/questions/586304\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "# os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/workspace\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Udacity Environment\n",
    "\n",
    "For learning purpose, let's find out what the Udacity virtual environment looks like. So in future should we wish to reproduce results outside the Udacity environment, we may be able to do some reverse-engineering - reproduce the Udacity environment (e.g. via Docker) and get our code working there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.3\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cli/1.16.17 Python/3.6.3 Linux/4.15.0-1083-gcp botocore/1.12.7\n"
     ]
    }
   ],
   "source": [
    "!aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Main EDA Notebook\n",
    "\n",
    "Our main EDA Notebook: `/home/workspace/EDA.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Utility Functions\n",
    "\n",
    "To make analysis more concise, we have refactered out some reusable Python procedures here (following some initial Notebook iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def spark_read_csv_raw(\n",
    "        spark: pyspark.sql.SparkSession,\n",
    "        file_path: str\n",
    "    ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV file into a Spark DataFrame. Read entire row into one column.\n",
    "    Useful for initila Data Quality Check.\n",
    "    Useful for deciding on beset delimiters to use.\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark\\\n",
    "        .read\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"delimiter\", \"\\n\")\\\n",
    "        .load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def spark_read_csv(\n",
    "        spark: pyspark.sql.SparkSession,\n",
    "        file_path: str, \n",
    "        delimiter: str = \",\", \n",
    "        header: bool = True\n",
    "    ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV file into a Spark DataFrame. Specify how we wish to read the CSV file.\n",
    "    This returns a SparkDataFrame that is useful for downstream analytics.\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark\\\n",
    "        .read\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"delimiter\", delimiter)\\\n",
    "        .option(\"header\", header)\\\n",
    "        .load(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def spark_df_overview(\n",
    "        spark_df: pyspark.sql.dataframe.DataFrame,\n",
    "        spark_df_name: str = \"spark_df_name_not_provided\",\n",
    "        top_x: int = 5\n",
    "    ) -> None:\n",
    "    \"\"\" Given a Spark DataFrame, print out useful high level summary \n",
    "    Ref: why use take() instead of collect():\n",
    "    https://sparkbyexamples.com/spark/show-top-n-rows-in-spark-pyspark/\n",
    "    \"\"\"\n",
    " \n",
    "    # Print the Spark DataFrame Name for ease of reading of console output\n",
    "    print(f\"Spark DataFrame Name: {spark_df_name}\")\n",
    "\n",
    "    # Total Column row\n",
    "    print(f\"Total Columns: {len(spark_df.columns)}\")\n",
    "\n",
    "    # Total row count\n",
    "    spark_df_rows = spark_df.count()\n",
    "    print(f\"Total Rows: {spark_df_rows}\")\n",
    "\n",
    "    # Basic Stricture of Spark DataFrame\n",
    "    print(\"Spark DataFrame Structure\")\n",
    "    spark_df.printSchema()\n",
    "    \n",
    "    # If dataframe has contents, print some samples\n",
    "    \n",
    "    df_top_x = spark_df.take(top_x)\n",
    "        \n",
    "    if spark_df_rows >= 1:\n",
    "        for row in range(top_x):\n",
    "            print(f\"*** Sample Row: {row} ***\")\n",
    "            print(df_top_x[row])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Where are the data\n",
    "\n",
    "The provided data can be found in the Udacity virtual workspace environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Data Domain 1: I94 Immigration Data (Primary)\n",
      "\n",
      "    * Dataset 1.1: I94 Immigration Data (2016?) - Sample CSV.\n",
      "        Path: raw_input_data/i94_sample_csv/immigration_data_sample.csv\n",
      "        \n",
      "    * Dataset 1.2: I94 Immigration Data (2016?) - Sample Parquet.\n",
      "        Path: sas_data/\n",
      "        Note that these Parquet files originated from SAS Datsets (1.3 below).\n",
      "        From analysis it appears that the Parquet Files (in path `sas_data`) come from the \n",
      "        Apr 2016 SAS dataset: `i94_apr16_sub.sas7bdat`. \n",
      "        (Of course! created by the Notebook: `Capstone Project Template.ipynb` - duh!)\n",
      "        \n",
      "    * Dataset 1.3: I94 Immigration Data (2016?) - SAS Datasets.\n",
      "        Path: /data/18-83510-I94-Data-2016\n",
      "        It contains 12 SAS Dataset (1 for each month in year 2016).\n",
      "        Each SAS Dataset has a naming convention like this: `i94_<mmm>16_sub.sas7bdat`,\n",
      "        where `mmm` is in this list `['apr','aug','dec','feb','jan','jul','jun','mar','may','nov','oct','sep']`\n",
      "        \n",
      "* Data Domain 2: World Temperature Data (Secondary)\n",
      "\n",
      "    * Dataset 2.1: World Temperature Data - CSV.\n",
      "        Path: /data2/GlobalLandTemperaturesByCity.csv\n",
      "        \n",
      "* Data Domain 3: US Cities Demographics (Secondary)\n",
      "\n",
      "    * Data Set 3.1: US Cities Demographics - CSV.\n",
      "        Path: raw_input_data/us_cities_demographics_csv/us-cities-demographics.csv\n",
      "    \n",
      "* Data Domain 4: Airport Codes (Secondary)\n",
      "\n",
      "    * Data Set 4.1: Airport Codes - CSV.\n",
      "        Path: raw_input_data/airport_codes_csv/airport-codes_csv.csv    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_desc = \\\n",
    "f\"\"\"\n",
    "* Data Domain 1: I94 Immigration Data (Primary)\n",
    "\n",
    "    * Dataset 1.1: I94 Immigration Data (2016?) - Sample CSV.\n",
    "        Path: {RAW_I94_SAMPLE}\n",
    "        \n",
    "    * Dataset 1.2: I94 Immigration Data (2016?) - Sample Parquet.\n",
    "        Path: {PROCESSED_I94_DIR}\n",
    "        Note that these Parquet files originated from SAS Datsets (1.3 below).\n",
    "        From analysis it appears that the Parquet Files (in path `sas_data`) come from the \n",
    "        Apr 2016 SAS dataset: `i94_apr16_sub.sas7bdat`. \n",
    "        (Of course! created by the Notebook: `Capstone Project Template.ipynb` - duh!)\n",
    "        \n",
    "    * Dataset 1.3: I94 Immigration Data (2016?) - SAS Datasets.\n",
    "        Path: {RAW_I94_MTHLY_SAS7BDAT_DIR}\n",
    "        It contains 12 SAS Dataset (1 for each month in year 2016).\n",
    "        Each SAS Dataset has a naming convention like this: `i94_<mmm>16_sub.sas7bdat`,\n",
    "        where `mmm` is in this list `['apr','aug','dec','feb','jan','jul','jun','mar','may','nov','oct','sep']`\n",
    "        \n",
    "* Data Domain 2: World Temperature Data (Secondary)\n",
    "\n",
    "    * Dataset 2.1: World Temperature Data - CSV.\n",
    "        Path: {RAW_GLOBAL_CITY_LAND_TEMPERATURE}\n",
    "        \n",
    "* Data Domain 3: US Cities Demographics (Secondary)\n",
    "\n",
    "    * Data Set 3.1: US Cities Demographics - CSV.\n",
    "        Path: {RAW_US_CITY_DEMOGRAPHICS}\n",
    "    \n",
    "* Data Domain 4: Airport Codes (Secondary)\n",
    "\n",
    "    * Data Set 4.1: Airport Codes - CSV.\n",
    "        Path: {RAW_AIRPORT_CODES}    \n",
    "\"\"\"\n",
    "\n",
    "print(data_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Domain 1: I94 Immigration Data (Primary)\n",
    "\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset 1.1: I94 Immigration Data (2016?) - Sample CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 142K Mar 15  2019 raw_input_data/i94_sample_csv/immigration_data_sample.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $RAW_I94_SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_immigration_sample_raw\n",
      "Total Columns: 1\n",
      "Total Rows: 1001\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(_c0=',cicid,i94yr,i94mon,i94cit,i94res,i94port,arrdate,i94mode,i94addr,depdate,i94bir,i94visa,count,dtadfile,visapost,occup,entdepa,entdepd,entdepu,matflag,biryear,dtaddto,gender,insnum,airline,admnum,fltno,visatype')\n",
      "*** Sample Row: 1 ***\n",
      "Row(_c0='2027561,4084316.0,2016.0,4.0,209.0,209.0,HHW,20566.0,1.0,HI,20573.0,61.0,2.0,1.0,20160422,,,G,O,,M,1955.0,07202016,F,,JL,56582674633.0,00782,WT')\n",
      "*** Sample Row: 2 ***\n",
      "Row(_c0='2171295,4422636.0,2016.0,4.0,582.0,582.0,MCA,20567.0,1.0,TX,20568.0,26.0,2.0,1.0,20160423,MTR,,G,R,,M,1990.0,10222016,M,,*GA,94361995930.0,XBLNG,B2')\n",
      "*** Sample Row: 3 ***\n",
      "Row(_c0='589494,1195600.0,2016.0,4.0,148.0,112.0,OGG,20551.0,1.0,FL,20571.0,76.0,2.0,1.0,20160407,,,G,O,,M,1940.0,07052016,M,,LH,55780468433.0,00464,WT')\n",
      "*** Sample Row: 4 ***\n",
      "Row(_c0='2631158,5291768.0,2016.0,4.0,297.0,297.0,LOS,20572.0,1.0,CA,20581.0,25.0,2.0,1.0,20160428,DOH,,G,O,,M,1991.0,10272016,M,,QR,94789696030.0,00739,B2')\n"
     ]
    }
   ],
   "source": [
    "# Read raw CSV into a Spark DataFrame where whole row is stored in with one column\n",
    "df_immigration_sample_raw = spark_read_csv_raw(\n",
    "    spark=spark,\n",
    "    file_path=RAW_I94_SAMPLE)\n",
    "\n",
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_immigration_sample_raw,\n",
    "    spark_df_name=\"df_immigration_sample_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_immigration_sample\n",
      "Total Columns: 29\n",
      "Total Rows: 1000\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(_c0='2027561', cicid='4084316.0', i94yr='2016.0', i94mon='4.0', i94cit='209.0', i94res='209.0', i94port='HHW', arrdate='20566.0', i94mode='1.0', i94addr='HI', depdate='20573.0', i94bir='61.0', i94visa='2.0', count='1.0', dtadfile='20160422', visapost=None, occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear='1955.0', dtaddto='07202016', gender='F', insnum=None, airline='JL', admnum='56582674633.0', fltno='00782', visatype='WT')\n",
      "*** Sample Row: 1 ***\n",
      "Row(_c0='2171295', cicid='4422636.0', i94yr='2016.0', i94mon='4.0', i94cit='582.0', i94res='582.0', i94port='MCA', arrdate='20567.0', i94mode='1.0', i94addr='TX', depdate='20568.0', i94bir='26.0', i94visa='2.0', count='1.0', dtadfile='20160423', visapost='MTR', occup=None, entdepa='G', entdepd='R', entdepu=None, matflag='M', biryear='1990.0', dtaddto='10222016', gender='M', insnum=None, airline='*GA', admnum='94361995930.0', fltno='XBLNG', visatype='B2')\n",
      "*** Sample Row: 2 ***\n",
      "Row(_c0='589494', cicid='1195600.0', i94yr='2016.0', i94mon='4.0', i94cit='148.0', i94res='112.0', i94port='OGG', arrdate='20551.0', i94mode='1.0', i94addr='FL', depdate='20571.0', i94bir='76.0', i94visa='2.0', count='1.0', dtadfile='20160407', visapost=None, occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear='1940.0', dtaddto='07052016', gender='M', insnum=None, airline='LH', admnum='55780468433.0', fltno='00464', visatype='WT')\n",
      "*** Sample Row: 3 ***\n",
      "Row(_c0='2631158', cicid='5291768.0', i94yr='2016.0', i94mon='4.0', i94cit='297.0', i94res='297.0', i94port='LOS', arrdate='20572.0', i94mode='1.0', i94addr='CA', depdate='20581.0', i94bir='25.0', i94visa='2.0', count='1.0', dtadfile='20160428', visapost='DOH', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear='1991.0', dtaddto='10272016', gender='M', insnum=None, airline='QR', admnum='94789696030.0', fltno='00739', visatype='B2')\n",
      "*** Sample Row: 4 ***\n",
      "Row(_c0='3032257', cicid='985523.0', i94yr='2016.0', i94mon='4.0', i94cit='111.0', i94res='111.0', i94port='CHM', arrdate='20550.0', i94mode='3.0', i94addr='NY', depdate='20553.0', i94bir='19.0', i94visa='2.0', count='1.0', dtadfile='20160406', visapost=None, occup=None, entdepa='Z', entdepd='K', entdepu=None, matflag='M', biryear='1997.0', dtaddto='07042016', gender='F', insnum=None, airline=None, admnum='42322572633.0', fltno='LAND', visatype='WT')\n"
     ]
    }
   ],
   "source": [
    "# Now read CSV again with additional config (Header, Delimiter, etc.)\n",
    "df_immigration_sample = spark_read_csv(\n",
    "    spark=spark,\n",
    "    file_path=RAW_I94_SAMPLE, \n",
    "    delimiter=\",\", \n",
    "    header=True)\n",
    "\n",
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_immigration_sample,\n",
    "    spark_df_name=\"df_immigration_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**User Notes**: the first column `_c0` is likely an identifier to the specific row of the full dataset? (e.g. a random row index of the originating full dataset?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### EDA - Dataset 1.2: I94 Immigration Data (2016?) - Parquet Files (originated from SAS Datsets?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 46M\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00001-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00002-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00003-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00004-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00005-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.2M Mar 19  2019 part-00006-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00007-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.2M Mar 19  2019 part-00008-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.2M Mar 19  2019 part-00009-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00010-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.3M Mar 19  2019 part-00011-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.2M Mar 19  2019 part-00012-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 3.6M Mar 19  2019 part-00013-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root    0 Mar 19  2019 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls -lh sas_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_parquet = spark.read.load(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_immigration_parquet\n",
      "Total Columns: 28\n",
      "Total Rows: 3096313\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1')\n",
      "*** Sample Row: 1 ***\n",
      "Row(cicid=5748518.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='NV', depdate=20591.0, i94bir=32.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1984.0, dtaddto='10292016', gender='F', insnum=None, airline='VA', admnum=94955622830.0, fltno='00007', visatype='B1')\n",
      "*** Sample Row: 2 ***\n",
      "Row(cicid=5748519.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20582.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956406530.0, fltno='00040', visatype='B1')\n",
      "*** Sample Row: 3 ***\n",
      "Row(cicid=5748520.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=29.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1987.0, dtaddto='10292016', gender='F', insnum=None, airline='DL', admnum=94956451430.0, fltno='00040', visatype='B1')\n",
      "*** Sample Row: 4 ***\n",
      "Row(cicid=5748521.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='WA', depdate=20588.0, i94bir=28.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='10292016', gender='M', insnum=None, airline='DL', admnum=94956388130.0, fltno='00040', visatype='B1')\n"
     ]
    }
   ],
   "source": [
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_immigration_parquet,\n",
    "    spark_df_name=\"df_immigration_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset 1.3: I94 Immigration Data (2016?) - SAS Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6.0G\n",
      "-rw-r--r-- 1 root root 451M May 31  2018 i94_apr16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 597M May 31  2018 i94_aug16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 500M May 31  2018 i94_dec16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 374M May 31  2018 i94_feb16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 415M May 31  2018 i94_jan16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 620M May 31  2018 i94_jul16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 684M May 31  2018 i94_jun16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 459M May 31  2018 i94_mar16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 501M May 31  2018 i94_may16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 424M May 31  2018 i94_nov16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 531M May 31  2018 i94_oct16_sub.sas7bdat\n",
      "-rw-r--r-- 1 root root 543M May 31  2018 i94_sep16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $RAW_I94_MTHLY_SAS7BDAT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Note that each SAS Dataset is quite large (about 0.5 GB each).\n",
    "# For now, let's just read in one and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_sas = spark\\\n",
    "    .read\\\n",
    "    .format('com.github.saurfang.sas.spark')\\\n",
    "    .load(f\"{RAW_I94_MTHLY_SAS7BDAT_DIR}/i94_apr16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_immigration_sas\n",
      "Total Columns: 28\n",
      "Total Rows: 3096313\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(cicid=6.0, i94yr=2016.0, i94mon=4.0, i94cit=692.0, i94res=692.0, i94port='XXX', arrdate=20573.0, i94mode=None, i94addr=None, depdate=None, i94bir=37.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu='U', matflag=None, biryear=1979.0, dtaddto='10282016', gender=None, insnum=None, airline=None, admnum=1897628485.0, fltno=None, visatype='B2')\n",
      "*** Sample Row: 1 ***\n",
      "Row(cicid=7.0, i94yr=2016.0, i94mon=4.0, i94cit=254.0, i94res=276.0, i94port='ATL', arrdate=20551.0, i94mode=1.0, i94addr='AL', depdate=None, i94bir=25.0, i94visa=3.0, count=1.0, dtadfile='20130811', visapost='SEO', occup=None, entdepa='G', entdepd=None, entdepu='Y', matflag=None, biryear=1991.0, dtaddto='D/S', gender='M', insnum=None, airline=None, admnum=3736796330.0, fltno='00296', visatype='F1')\n",
      "*** Sample Row: 2 ***\n",
      "Row(cicid=15.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='WAS', arrdate=20545.0, i94mode=1.0, i94addr='MI', depdate=20691.0, i94bir=55.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='T', entdepd='O', entdepu=None, matflag='M', biryear=1961.0, dtaddto='09302016', gender='M', insnum=None, airline='OS', admnum=666643185.0, fltno='93', visatype='B2')\n",
      "*** Sample Row: 3 ***\n",
      "Row(cicid=16.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=28.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468461330.0, fltno='00199', visatype='B2')\n",
      "*** Sample Row: 4 ***\n",
      "Row(cicid=17.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=4.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=2012.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468463130.0, fltno='00199', visatype='B2')\n"
     ]
    }
   ],
   "source": [
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_immigration_sas,\n",
    "    spark_df_name=\"df_immigration_sas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Domain 2: World Temperature Data (Secondary)\n",
    "\n",
    "This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dataset 2.1: World Temperature Data - CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 1002 1003 509M Mar 30  2019 /data2/GlobalLandTemperaturesByCity.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $RAW_GLOBAL_CITY_LAND_TEMPERATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_temperature_raw\n",
      "Total Columns: 1\n",
      "Total Rows: 8599213\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(_c0='dt,AverageTemperature,AverageTemperatureUncertainty,City,Country,Latitude,Longitude')\n",
      "*** Sample Row: 1 ***\n",
      "Row(_c0='1743-11-01,6.068,1.7369999999999999,Århus,Denmark,57.05N,10.33E')\n",
      "*** Sample Row: 2 ***\n",
      "Row(_c0='1743-12-01,,,Århus,Denmark,57.05N,10.33E')\n",
      "*** Sample Row: 3 ***\n",
      "Row(_c0='1744-01-01,,,Århus,Denmark,57.05N,10.33E')\n",
      "*** Sample Row: 4 ***\n",
      "Row(_c0='1744-02-01,,,Århus,Denmark,57.05N,10.33E')\n"
     ]
    }
   ],
   "source": [
    "# Read raw CSV into a Spark DataFrame where whole row is stored in with one column\n",
    "df_temperature_raw = spark_read_csv_raw(\n",
    "    spark=spark,\n",
    "    file_path=RAW_GLOBAL_CITY_LAND_TEMPERATURE)\n",
    "\n",
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_temperature_raw,\n",
    "    spark_df_name=\"df_temperature_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_temperature\n",
      "Total Columns: 7\n",
      "Total Rows: 8599212\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(dt='1743-11-01', AverageTemperature='6.068', AverageTemperatureUncertainty='1.7369999999999999', City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E')\n",
      "*** Sample Row: 1 ***\n",
      "Row(dt='1743-12-01', AverageTemperature=None, AverageTemperatureUncertainty=None, City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E')\n",
      "*** Sample Row: 2 ***\n",
      "Row(dt='1744-01-01', AverageTemperature=None, AverageTemperatureUncertainty=None, City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E')\n",
      "*** Sample Row: 3 ***\n",
      "Row(dt='1744-02-01', AverageTemperature=None, AverageTemperatureUncertainty=None, City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E')\n",
      "*** Sample Row: 4 ***\n",
      "Row(dt='1744-03-01', AverageTemperature=None, AverageTemperatureUncertainty=None, City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E')\n"
     ]
    }
   ],
   "source": [
    "# Now read CSV again with additional config (Header, Delimiter, etc.)\n",
    "df_temperature = spark_read_csv(\n",
    "    spark=spark,\n",
    "    file_path=RAW_GLOBAL_CITY_LAND_TEMPERATURE, \n",
    "    delimiter=\",\", \n",
    "    header=True)\n",
    "\n",
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_temperature,\n",
    "    spark_df_name=\"df_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Domain 3: US Cities Demographics (Secondary)\n",
    "\n",
    "This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset 3.1: US Cities Demographics - CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 246K Mar 15  2019 raw_input_data/us_cities_demographics_csv/us-cities-demographics.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $RAW_US_CITY_DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_us_cities_demo_raw\n",
      "Total Columns: 1\n",
      "Total Rows: 2892\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(_c0='City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count')\n",
      "*** Sample Row: 1 ***\n",
      "Row(_c0='Silver Spring;Maryland;33.8;40601;41862;82463;1562;30908;2.6;MD;Hispanic or Latino;25924')\n",
      "*** Sample Row: 2 ***\n",
      "Row(_c0='Quincy;Massachusetts;41.0;44129;49500;93629;4147;32935;2.39;MA;White;58723')\n",
      "*** Sample Row: 3 ***\n",
      "Row(_c0='Hoover;Alabama;38.5;38040;46799;84839;4819;8229;2.58;AL;Asian;4759')\n",
      "*** Sample Row: 4 ***\n",
      "Row(_c0='Rancho Cucamonga;California;34.5;88127;87105;175232;5821;33878;3.18;CA;Black or African-American;24437')\n"
     ]
    }
   ],
   "source": [
    "# Read raw CSV into a Spark DataFrame where whole row is stored in with one column\n",
    "df_us_cities_demo_raw = spark_read_csv_raw(\n",
    "    spark=spark,\n",
    "    file_path=RAW_US_CITY_DEMOGRAPHICS)\n",
    "\n",
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_us_cities_demo_raw,\n",
    "    spark_df_name=\"df_us_cities_demo_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_us_cities_demo\n",
      "Total Columns: 12\n",
      "Total Rows: 2891\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(City='Silver Spring', State='Maryland', Median Age='33.8', Male Population='40601', Female Population='41862', Total Population='82463', Number of Veterans='1562', Foreign-born='30908', Average Household Size='2.6', State Code='MD', Race='Hispanic or Latino', Count='25924')\n",
      "*** Sample Row: 1 ***\n",
      "Row(City='Quincy', State='Massachusetts', Median Age='41.0', Male Population='44129', Female Population='49500', Total Population='93629', Number of Veterans='4147', Foreign-born='32935', Average Household Size='2.39', State Code='MA', Race='White', Count='58723')\n",
      "*** Sample Row: 2 ***\n",
      "Row(City='Hoover', State='Alabama', Median Age='38.5', Male Population='38040', Female Population='46799', Total Population='84839', Number of Veterans='4819', Foreign-born='8229', Average Household Size='2.58', State Code='AL', Race='Asian', Count='4759')\n",
      "*** Sample Row: 3 ***\n",
      "Row(City='Rancho Cucamonga', State='California', Median Age='34.5', Male Population='88127', Female Population='87105', Total Population='175232', Number of Veterans='5821', Foreign-born='33878', Average Household Size='3.18', State Code='CA', Race='Black or African-American', Count='24437')\n",
      "*** Sample Row: 4 ***\n",
      "Row(City='Newark', State='New Jersey', Median Age='34.6', Male Population='138040', Female Population='143873', Total Population='281913', Number of Veterans='5829', Foreign-born='86253', Average Household Size='2.73', State Code='NJ', Race='White', Count='76402')\n"
     ]
    }
   ],
   "source": [
    "# Now read CSV again with additional config (Header, Delimiter, etc.)\n",
    "df_us_cities_demo = spark_read_csv(\n",
    "    spark=spark,\n",
    "    file_path=RAW_US_CITY_DEMOGRAPHICS, \n",
    "    delimiter=\";\", \n",
    "    header=True)\n",
    "\n",
    "# Print Spark DataFrame overview\n",
    "spark_df_overview(\n",
    "    spark_df=df_us_cities_demo,\n",
    "    spark_df_name=\"df_us_cities_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Domain 4: Airport Codes (Secondary)\n",
    "\n",
    "This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dataset 4.1: Airport Codes - CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 5.8M Mar 15  2019 raw_input_data/airport_codes_csv/airport-codes_csv.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $RAW_AIRPORT_CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_airport_raw\n",
      "Total Columns: 1\n",
      "Total Rows: 55076\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(_c0='ident,type,name,elevation_ft,continent,iso_country,iso_region,municipality,gps_code,iata_code,local_code,coordinates')\n",
      "*** Sample Row: 1 ***\n",
      "Row(_c0='00A,heliport,Total Rf Heliport,11,NA,US,US-PA,Bensalem,00A,,00A,\"-74.93360137939453, 40.07080078125\"')\n",
      "*** Sample Row: 2 ***\n",
      "Row(_c0='00AA,small_airport,Aero B Ranch Airport,3435,NA,US,US-KS,Leoti,00AA,,00AA,\"-101.473911, 38.704022\"')\n",
      "*** Sample Row: 3 ***\n",
      "Row(_c0='00AK,small_airport,Lowell Field,450,NA,US,US-AK,Anchor Point,00AK,,00AK,\"-151.695999146, 59.94919968\"')\n",
      "*** Sample Row: 4 ***\n",
      "Row(_c0='00AL,small_airport,Epps Airpark,820,NA,US,US-AL,Harvest,00AL,,00AL,\"-86.77030181884766, 34.86479949951172\"')\n"
     ]
    }
   ],
   "source": [
    "df_airport_raw = spark_read_csv_raw(\n",
    "    spark=spark,\n",
    "    file_path=RAW_AIRPORT_CODES)\n",
    "\n",
    "spark_df_overview(\n",
    "    spark_df=df_airport_raw,\n",
    "    spark_df_name=\"df_airport_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Name: df_airport\n",
      "Total Columns: 12\n",
      "Total Rows: 55075\n",
      "Spark DataFrame Structure\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "*** Sample Row: 0 ***\n",
      "Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125')\n",
      "*** Sample Row: 1 ***\n",
      "Row(ident='00AA', type='small_airport', name='Aero B Ranch Airport', elevation_ft='3435', continent='NA', iso_country='US', iso_region='US-KS', municipality='Leoti', gps_code='00AA', iata_code=None, local_code='00AA', coordinates='-101.473911, 38.704022')\n",
      "*** Sample Row: 2 ***\n",
      "Row(ident='00AK', type='small_airport', name='Lowell Field', elevation_ft='450', continent='NA', iso_country='US', iso_region='US-AK', municipality='Anchor Point', gps_code='00AK', iata_code=None, local_code='00AK', coordinates='-151.695999146, 59.94919968')\n",
      "*** Sample Row: 3 ***\n",
      "Row(ident='00AL', type='small_airport', name='Epps Airpark', elevation_ft='820', continent='NA', iso_country='US', iso_region='US-AL', municipality='Harvest', gps_code='00AL', iata_code=None, local_code='00AL', coordinates='-86.77030181884766, 34.86479949951172')\n",
      "*** Sample Row: 4 ***\n",
      "Row(ident='00AR', type='closed', name='Newport Hospital & Clinic Heliport', elevation_ft='237', continent='NA', iso_country='US', iso_region='US-AR', municipality='Newport', gps_code=None, iata_code=None, local_code=None, coordinates='-91.254898, 35.6087')\n"
     ]
    }
   ],
   "source": [
    "df_airport = spark_read_csv(\n",
    "    spark=spark,\n",
    "    file_path=RAW_AIRPORT_CODES, \n",
    "    delimiter=\",\", \n",
    "    header=True)\n",
    "\n",
    "spark_df_overview(\n",
    "    spark_df=df_airport,\n",
    "    spark_df_name=\"df_airport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Mini Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We now have a feel of the raw datasets provided by Udacity. We also have successfully read these raw files into Spark DataFrames for downstream processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
